%TODO Möglicherweise Vorkommen von "Schemata" durch "Datenflussschemata" ersetzen
%TODO Auch auf Memory Encryption Engine bei Datenverarbeitung eingehen!!
\chapter{Evaluierung}

Wie bereits in den vorangegangenen Kapiteln betrachtet wurde, verspricht Intel \ac{SGX} das Aufweisen guter Sicherheitsmerkmale und eine leichte Systemintegration. Obwohl der herkömmliche Einsatzzweck eher auf die reine Speicherung sensibler Daten beruht, ist es das Ziel dieser Arbeit zu untersuchen, ob es sinnvoll ist, Intel \ac{SGX} im Zuge der Anfrageverarbeitung in einem hauptspeicherbasierten Datenbanksystem einzusetzen. In diesem Kapitel erfolgt eine Bewertung dieses Anwendungsfalls anhand von Leistungsbenchmarks verschiedener Szenarien der Datenverarbeitung.

Zunächst wird hierzu eine Einführung in das Konzept der Untersuchungen gegeben. Die darauffolgenden Abschnitte befassen sich jeweils mit den konkreten Punkten des Testplans, wobei die erfassten Ergebnisse gezeigt und Beobachtungen getroffen werden. In einem abschließenden Fazit erfolgt eine Auswertung jener Ergebnisse.

\section{Konzept}

Der folgende Abschnitt befasst sich mit dem konzeptionellen Vorgehen bei der Durchführung der einzelnen Untersuchungen. Es wird zunächst der Frage nachgegangen, welche Tests in dem vorliegenden Kontext sinnvoll sind. Somit liegt ein konkreter Testplan vor, dessen Ergebnisse im Laufe des Kapitels sukzessive beschrieben werden können. Im zweiten Abschnitt wird die eingesetzte Testumgebung aus Sicht von Soft- und Hardware beschrieben. Hierbei wird vor allem kurz auf wichtige Herangehensweisen in der Implementierung eingegangen.

\subsection{Untersuchungen}

Die Datenverarbeitung innerhalb der Enclave wird im Folgenden durch das allgemeine Schema des Datenflusses in Abbildung \ref{fig:scenarios} modelliert.
\begin{figure}[h]
	\includegraphics[width=0.9\linewidth]{img/EvalScenarios.pdf}
	\centering
	\caption{Allgemeines Szenario der Datenverarbeitung}
	\label{fig:scenarios}
\end{figure} 
Es beinhaltet die einzelnen Teilschritte, welche zu einem vollständigen Berechnungsprozess gehören, beginnend mit dem Transfer von gespeicherten und Basisdaten in die sichere Komponente der Enclave. In unserem Kontext liegt ein nicht vertrauenswürdiges Umfeld vor. Es wird daher die Annahme getroffen, dass jene Basisdaten stets in einer verschlüsselten Form vorliegen. Die Zwischenschritte ergeben sich dann wie folgt:

\paragraph{Kopieren}
Die Basisdaten werden zu Beginn in die Enclave transferiert. Auf technischer Ebene erfolgt dies durch ein Kopieren des Buffers in den gesicherten Speicher. Zum Ende der Verarbeitung erfolgt ein entsprechendes Zurückkopieren des Endergebnisses in die unsichere Domäne.

\paragraph{Entschlüsseln/Verschlüsseln}
Sobald sich die Daten in der Enclave befinden, müssen sie zunächst unter Hinzunahme eines geheimen, gespeicherten Schlüsseln entschlüsselt werden, um eine Verarbeitung zu ermöglichen. Nachdem die Arbeit auf den Daten beendet wurde, wird wieder eine Verschlüsselung vorgenommen, um die ausgehenden Daten zu schützen.

\paragraph{Verarbeiten}
Die eigentliche Verarbeitung findet auf den Klartextdaten statt und beinhaltet eine einzelne oder Abfolge von Operationen. Im zweiten Fall entstehen unverschlüsselte Zwischenergebnisse.

\paragraph{}
Die Schritte vor und nach der eigentlichen Verarbeitung sind offensichtlich mit einem gewissen rechnerischen Aufwand verbunden. Von daher ist es erstrebenswert, möglichst viele der zu verarbeitenden Daten auf einmal zu Kopieren und in der Enclave zu halten. Demgegenüber steht allerdings der stark eingeschränkte Speicherplatz, welcher im \ac{PRM} zur Verfügung steht. Ein Weg, dem entgegenzuwirken ist der Einsatz von den in Kapitel 3 beschriebenen Kompressionsverfahren. Sie ermöglichen eine Reduzierung der Datenmenge im sicheren Bereich auf Kosten eines weiteren Berechnungsschrittes je für die Komprimierung und Dekomprimierung. Nach Zunahme dieser Schritte ergibt sich das Schema in Abbildung \ref{fig:scenariocomp} Je nach Art der eigentlichen Berechnung ist es mitunter möglich, direkt auf den komprimierten Daten zu arbeiten. Dazu wurde beispielsweise die Bildung des Summenaggregats auf einer lauflängenkodierten Menge betrachtet. Sollte dies nicht der Fall sein, beinhaltet die Menge an Operationen im Verarbeitungsschritt weitere (De)komprimierungsschritte.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/EvalScenariosComp.pdf}
	\centering
	\caption{Szenario der Datenverarbeitung mit (De)komprimierung}
	\label{fig:scenariocomp}
\end{figure}

Ebenso denkbar ist, dass die Basisdaten bereits in einer komprimierten Form vorliegen, wie dies in einer In-Memory Datenbank durchaus üblich ist. In diesem Fall ist ein Kompressionsschritt vor der Verarbeitung nicht obligatorisch. Stattdessen kann sogar erst eine Dekomprimierung stattfinden, sollte es nicht anders möglich sein die Berechnungen durchzuführen. Denkbar ist dann auch eine Kompression unter Nutzung eines anderen Verfahrens oder eine Doppelte. Ein Vorteil ist dadurch gegeben, dass kleinere Datenmengen in die Enclave kopiert werden müssen, was außerdem in einer schnelleren Entschlüsselung resultiert. Nach der Verarbeitung kann es durchaus möglich sein, dass die Daten auch weiterhin komprimiert bleiben. Üblicherweise findet hier jedoch wieder eine Dekomprimierung der Ergebnisdaten in die ursprüngliche Repräsentation statt, bevor es zur Verschlüsselung und dem Kopieren geht.

Der Vollständigkeit wegen muss an dieser Stelle auch angemerkt werden, dass eigenschaftserhaltende Verschlüsselungsverfahren ihren Einsatz finden können, um einen Teil des Aufwandes durch Ver- bzw. Entschlüsselungen einzusparen. Ein mögliches Szenario wäre das Auftreten von Speicherknappheit in der Enclave, wodurch ein Teil der Daten zunächst ausgelagert werden müsste. Da ohnehin eine vorausgehende Verschlüsselung notwendig ist, kann beispielsweise ein deterministisches Verschlüsselungsschema angewandt werden. Dies erlaubt eine parallele Bearbeitung außerhalb der Enclave, bevor es zu einem erneuten Transfer einschließlich Kopiervorgang und Verschlüsselung kommt. Im Falle des deterministischen Verfahrens wäre es etwa möglich, Gleichheitsprädikate zu prüfen und einen Join zwischen zwei Spalten durchzuführen. 

Es wird klar, dass es durchaus eine große Fülle von weiteren Szenarien geben kann. In Bezug auf die Untersuchungen wird sich daher auf die geläufigsten beschränkt. Bei Betrachtung der beiden aufgeführten Datenflussschemata sind stets zwei Kopiervorgänge am Gesamtablauf beteiligt. In Kapitel 4 wurden diese als Sicherheitsmaßnahme vorgestellt, welche durch die Edge Routines realisiert wird. Mit dem Setzen des user\textunderscore check Attributs an den Funktionsargumenten wurde zudem ein Mechanismus vorgestellt, welcher ein direktes Arbeiten auf den Daten ohne Kopieren ermöglicht. Demzufolge ergibt sich das in Abbildung \ref{fig:scenarionocopy} gezeigte Schema. Beide Varianten werden zunächst ohne einen konkreten Verarbeitungsschritt verglichen. Die beiden hierzu getätigten Tests werden als enclaveinterne Untersuchungen zusammengefasst. Das Ziel ist es, herauszufinden, wie groß der durch das Kopieren der Buffer entstehende Overhead ist.

\begin{figure}[h]
	\includegraphics[width=0.7\linewidth]{img/EvalScenariosNoCopy.pdf}
	\centering
	\caption{Szenario der Datenverarbeitung ohne Kopiervorgänge}
	\label{fig:scenarionocopy}
\end{figure}

Die darauffolgende Reihe an Tests beschäftigt sich mit dem Aspekt der konkreten Datenverarbeitung. Es wird dabei jeweils ein Ablauf in der Enclave mit einer gleichen Abfolge innerhalb eines regulären Programms verglichen. Anstelle des Verarbeitungsschrittes werden die entsprechende Operationen eingesetzt. Angefangen wird mit einer einfachen Iteration über die Eingabemenge als grundlegenden Baustein vieler komplexerer Operationen. Daraufhin werden jeweils die Kompression und Dekompression mittels VByte und Lauflängenkodierung als leichtgewichtige Kompressionsverfahren untersucht.

Der Kernpunkt dieser Untersuchungen ist herauszufinden, ob die Leistung signifikant durch die sicheren Berechnungen beeinflusst wird. Interessant ist auch, ob die einzelnen Verfahren gewisse Merkmale aufweisen, welche eine Verarbeitung mittels \ac{SGX} zusätzlich erschweren. Die Zwischenschritte zur Ent- und Verschlüsselung werden zunächst außer Acht gelassen, um einen besseren Eindruck über die jeweilige Operation und das Zusammenspiel mit dem im Falle der Enclave auftretenden Kopiervorgang zu bekommen.

Die dritte Menge von Untersuchungen widmet sich hingegen einer Einbeziehung der kryptographischen Funktionalität. Im Vordergrund steht dessen Kombination mit einer verarbeitenden Operation. Um zu testen, wie die Leistung der kryptographischen Algorithmen abschneidet, wird als erstes das in Abbildung \ref{fig:scenarionocopy} aufgegriffen, in einem herkömmlichen Umfeld durchgeführt und mit dem Verlauf in der Enclave verglichen. Daraufhin wird jeweils die Kompression durch VByte mit der Ent- und Verschlüsselung kombiniert. Um ein Gesamtbild von der Leistung unter \ac{SGX} zu erhalten, wird abschließend ein komplexer Vorgang getestet. Das Ziel ist es herauszufinden, wie gut die Technologie in einer umfangreicheren Berechnung abschneidet.
%TODO Mitunter ein paar der oberen Details in die einzelnen Beschreibungen der Settings auslagern
\subsection{Testumgebung}
%TODO Es wurden Mittelwerte gebildet mit Fenstergröße 50
%TODO Auf Enclave Konfiguration eingehen
Die Umsetzung der Testumgebung erfolgte mit dem Ziel, die einheitliche Durchführung jeglicher Untersuchungen zu erlauben. Wichtig war von daher eine genaue Festlegung des zu bewertenden Sachverhalts. Da es sich bei sämtlichen Tests um Messungen in Bezug auf die Leistung (Performance Benchmarks) handelt, welche im Kontext der Datenverarbeitung stattfindet, wurde das gängige Maß \ac{MIPS} gewählt. In diesem Fall orientiert es sich aber nicht auf die durchgeführten Prozessorinstruktionen, sondern misst verarbeitete Byte, genauer gesagt Megabyte je Sekunde. 

Auf technischer Ebene ist jeder zu testende Ablauf durch eine Funktion realisiert, weshalb beide Begriffe nachfolgend synonym gebraucht werden. Während des Testdurchgangs wird der jeweilige Ablauf auf einer der Größe nach kontinuierlich steigenden Menge an Eingabedaten ausgeführt. Alle getesteten Funktionen sind daher, wie in der Praxis üblich in der Lage, variable Datenmengen entgegenzunehmen und zu verarbeiten (Bulk Processing). Es ergibt sich ein Verlauf der gemessenen \ac{MIPS} je Größe des Eingabebuffers. Die unterschiedliche Natur der Funktionen bedingt jedoch, dass es keine einheitliche Schrittfolge geben kann, nach welcher sich die Eingabegröße erhöht. Kompressionen durch VByte und Lauflängenkodierung erfordern etwa 32 Bit (4 Byte) Integer Werte als Eingabe, während die Durchführung einer einfachen Iteration byteweise erfolgen kann. Um dennoch eine kontinuierliche Steigung der Eingabemenge zu ermöglichen, wurde die Schrittfolge entsprechend für jeden Test angepasst, während die Anzahl unterschiedlicher Ausprägungen von Eingabewerten einer konkreten Festlegung entspricht. Gegeben ist eine Funktion mit konstanter Schrittlänge $s$ in Byte. Die Größe der Eingabedaten $d$ (Byte) im Schritt $i$ ergibt sich wie folgt: 

\begin{equation*}
	d = i * s; i \in [0, 25000]
\end{equation*}

Um Schwankungen in den Berechnungen, etwa aufgrund des Schedulings entgegenzuwirken, wurde jeweils pro Ausprägung des Eingabewertes ein Mittelwert der \ac{MIPS} aus 200 Ausführungen gebildet. Der Eingabebuffer wird vor deren Ausführung mit Testdaten gefüllt, welche je nach Testfall von unterschiedlicher Form sind.

Die beschriebene Methodik zur Leistungsanalyse wurde in eine gesonderte Bibliothek ausgelagert. Zum Zwecke der Untersuchungen wurden zudem zwei unabhängige C++ Anwendungen erstellt. Erstere bindet eine Enclave ein, in welcher die zu testenden Operationen umgesetzt sind. Über jene Anwendung erfolgen die Messungen der sicheren Domäne. Die zweite ist ein Gegenstück ohne die Nutzung von \ac{SGX}. In diesem Fall sind die Abläufe auf gewöhnliche Art implementiert. Die zum Einsatz gekommenen Kompressionsalgorithmen wurden zum Zwecke der Untersuchungen zur Verfügung gestellt. Beide Anwendungen nutzten dahingegen die \textit{Intel Integrated Performance Primitives Cryptography} Bibliothek zur Einbettung der benötigten kryptographischen Algorithmen. Konkret kam in allen entsprechenden Tests eine hardwarebeschleunigte Implementierung von \ac{AES} im Modus \ac{CBC} und der Schlüssellänge 128 Bit zum Einsatz. Kompiliert wurden beide Programme mit dem Intel C++ Compiler in der Version 17. Jegliche Compileroptimierungen wurden jeweils zur Gewährleistung eines fairen und sauberen Testergebnisses deaktiviert. 

Alle Untersuchungen fanden unter Windows 10 auf der gleichen Hardware statt. Das System verfügte über 16 GB Hauptspeicher und als Prozessor diente ein Intel Core i5-7600 (Kaby Lake) mit 3,50 GHz Taktfrequenz.

\section{Enclaveinterne Untersuchungen}

Die erste Reihe an Untersuchungen ist zunächst sehr grundlegend und vergleichsweise einfach aufgebaut. Sie beschäftigt sich mit dem Vergleich der Ansätze zur Vor- und Nachbereitung der eigentlichen Datenverarbeitung in einer Enclave. Im Vordergrund steht das durch die Edge Routines durchgesetzte Kopieren der Ein- und Ausgabedaten zwischen der Enclave und unsicheren Anwendung, welche ihre Dienste beansprucht. Verglichen wird dies mit einem Vorgehen, in welchem das Kopieren durch Setzen des user\textunderscore check Attributes in der Enclavedefinition entfällt. Wie bereits im vorangegangenen Kapitel beschrieben, ist jene Maßnahme nur mit äußerster Vorsicht zu treffen.

\paragraph{Kopieren der Daten in und aus Enclave}

Grundlage der ersten Untersuchung sind die beiden in Abbildung \ref{fig:settingeval1} dargestellten Schemata. Auf der linken Seite ist der Verlauf dargestellt, der ein Kopieren von Eingabedaten in die Enclave beinhaltet. Der eigentliche Funktionsaufruf enthält keinerlei Verarbeitungen und kehrt sofort zurück, was ein abschließendes Kopieren der leeren Ausgabedaten zur Folge hat. Auf diese Weise kann der Kopiervorgang unabhängig von der eigentlichen, in der Enclave durchzuführenden Funktionalität untersucht werden. Die Größe des Ausgabebuffers ist jeweils stets gleich jener der Eingabe. Das Schema auf der rechten Seite sieht keinen Kopiervorgang vor, womit nur ein einfacher \ac{ECall} verbleibt. Obwohl dieser ebenso durch die Edge Routines realisiert wird, so übernehmen diese keinerlei sonstige Berechnungen. Die vorliegende Form der Daten ist für diese Tests vollkommen ohne Relevanz, weshalb etwa eine Verschlüsselung nicht berücksichtigt wurde. Das Ergebnis der beiden Durchgänge ist in Abbildung \ref{fig:eval1} zu sehen.

\begin{figure}[h]
	\includegraphics[width=0.9\linewidth]{img/SettingEval1.pdf}
	\centering
	\caption{Datenflussschemata der ersten Untersuchung}
	\label{fig:settingeval1}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval1.pdf}
	\centering
	\caption{Vergleich Kopieren der Daten in die Enclave und kein Kopieren mittels user\textunderscore check}
	\label{fig:eval1}
\end{figure}

Erwartungsgemäß verläuft die blaue Kurve konstant linear steigend. Dies ist damit zu begründen, dass  gar keine Abhängigkeit von der eigentlichen Eingabemenge besteht. Der einzige limitierende Faktor ist ein konstanter Overhead durch den \ac{ECall} und des damit verbundenen Kontrollflusses durch die Edge Routines. Je größer die Eingabedaten werden, desto höher ist folglich die Zahl an \ac{MIPS}. Die grüne Kurve verzeichnet einen Verlauf, dessen Steigung mit zunehmender Anzahl an Byte gleichmäßig abnimmt und sich einem Grenzwert von etwa 6000 \ac{MIPS} annähert. Somit bleibt sie im Vergleich immer weiter zurück und die Steigung der Differenzkurve nähert sich zunehmend jener der Blauen. Dieser Verlauf wird im Folgenden als Anlaufphase bezeichnet. Idealerweise würde die Kurve von Beginn an einen konstanten Wert aufweisen, da die in den Edge Routines ausgeführten Operationen wie das Kopieren der Buffer idealerweise eine lineare Komplexität aufweisen. Entgegen wirkt jedoch der konstante Aufwand, welcher durch den Aufruf des \acp{ECall} entsteht, etwa durch die damit verbundenen Überprüfungen, bzw. Funktionsaufrufe an sich. Jener Aufwand ist im Verhältnis zur eigentlichen, linearen Verarbeitung durch das Kopieren anfangs noch sehr groß. Mit zunehmender Eingabemenge fällt der konstante Anteil an der Berechnungszeit jedoch immer kleiner aus und wird vernachlässigbar, wodurch sich der Prozess seiner tatsächlichen Verarbeitungsrate von ca. 6000 \ac{MIPS} nähert.

\paragraph{Kopieren sowie Ent- und Verschlüsselung}

Während die vorige Untersuchung lediglich ein Kopieren auf beliebigen Daten beinhaltete, so wird im Folgenden davon ausgegangen, dass diese zusätzlich eine verschlüsselte Form aufweisen. Das Modell wird daher um je einen Schritt zur Entschlüsselung nach dem Eintritt in die Enclave sowie einen Verschlüsselungsschritt vor dem Verlassen ergänzt. Im Mittelpunkt steht auch weiterhin ein Vergleich der Verarbeitungsgeschwindigkeit des Ablaufs mit und ohne Kopieren. Somit ergeben sich die in Abbildung \ref{fig:settingeval2} gezeigten Schemata.
% TODO Oben rein: Es wurde eine Schrittgröße von 16 Byte gewählt (128 Bit), da dies der Blockgröße des eingesetzten AES Verschlüsselungsverfahrens entspricht. Auf diese Weise steigt der Aufwand je Ausprägung von Eingabewert definitiv an. Ansonsten Padding dazwischen.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/SettingEval2.pdf}
	\centering
	\caption{Datenflussschemata der zweiten Untersuchung}
	\label{fig:settingeval2}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval2.pdf}
	\centering
	\caption{Vergleich Ent- und Verschlüsselung mit und ohne Kopiervorgang}
	\label{fig:eval2}
\end{figure}

Das in Abbildung \ref{fig:eval2} gezeigte Ergebnis unterscheidet sich entsprechend der zusätzlichen Berechnungen von dem der ersten Untersuchung. Das hängt damit zusammen, dass nun in beiden Durchgängen eine echte Relation zwischen Datenmenge und Berechnungsaufwand existiert. Entsprechend des Verschlüsselungsverfahrens bzw. Operationsmodus ist diese wie erwartet linearer Natur. Es kann auch hier die Beobachtung getroffen werden, dass eine Anlaufphase auftritt, bevor der konstante Wert erreicht wird. Da der dafür verantwortliche \ac{ECALL} jedoch in beiden Fällen etwa den gleichen konstant andauernden Anteil aufweist, dauert jene Phase jeweils bis zu der gleichen Datenmenge von ca. 100.000 Byte an. Im Differenzgraphen lässt sich ein gleichmäßiger Offset von ungefähr 100 \ac{MIPS} beobachten, welcher durch die Kopiervorgänge und Überprüfungen im linken Ablauf des Untersuchungsschemas entsteht.

\section{Einfache Datenverarbeitung}

Die nun folgenden Untersuchungen befassen sich mit verschiedenen datenverarbeitenden Verfahren. Jene werden zum einen innerhalb der Enclave und außerdem in einer regulären Anwendung durchgeführt. Der Vergleich beider Ergebnisse liefert jeweils eine Aussage über die zu erwartende Leistung des Verfahrens, sollte dessen Ausführung in eine gesicherte Umgebung ausgelagert werden. Allgemein lassen sich die getroffenen Untersuchungen durch die in Abbildung \ref{fig:settingsection2} gezeigten Schemata zusammenfassen. Links ist der jeweilige Durchgang dargestellt, welcher eine Enclave einbezieht. Die ein- und ausgehenden Daten werden stets wie üblich kopiert. Um den Schwerpunkt auf die Verarbeitungsverfahren zu legen, entfallen jegliche kryptographische Operationen. Im zentralen Zwischenschritt wird je Untersuchung ein konkretes Verfahren eingesetzt. Auf der rechten Seite geschieht der Ablauf durch die Ausführung des Verfahrens auf einem gegebenen Buffer zur Eingabe und ein Ablegen des Resultats in einem entsprechenden Ausgabebuffer.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/SettingSection2.pdf}
	\centering
	\caption{Untersuchungsschemata der einfachen Datenverarbeitung}
	\label{fig:settingsection2}
\end{figure}

\paragraph{Iteration über Datenmenge}

Als erstes wird mit der Iteration über die Eingabemenge ein Verfahren untersucht, welches in der Datenverarbeitung sehr gängig ist. Seinen Einsatz findet es etwa bei der Aggregation von Spalten zur Summen- oder Durchschnittsbildung, aber beispielsweise auch bei der Prüfung von Prädikaten im Zuge eines Joins zweier Spalten. Entsprechend bildet es stets nur einen grundlegenden Bestandteil einer komplexeren Operation. Für den Test wird die komplette Eingabedatenmenge byteweise iteriert, während der aktuelle Wert einer Variablen zugewiesen wird. Somit erfolgt eine Erhöhung der Datenmenge um ein Byte je Schritt. Als Ausgabe dient lediglich ein leerer Buffer. Abbildung \ref{fig:eval3} zeigt das entstandene Ergebnis.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval3.pdf}
	\centering
	\caption{Vergleich Iteration über Datenmengen}
	\label{fig:eval3}
\end{figure}

Es ist diesmal gut zu erkennen, dass die Anlaufphasen der beiden Kurven sehr unterschiedlich verlaufen. Die Berechnung außerhalb einer Enclave erreicht schon sehr zeitig bei einer Datenmenge von 5000 Byte seine tatsächliche Verarbeitungsrate von 750 \ac{MIPS}. Die grüne Kurve hingegen weist erst ab ca. 25.000 Byte einen ungefähr konstanten Wert auf. Der \ac{ECall} besteht insgesamt aus mehreren Funktionsaufrufen durch die Edge Routines, dessen Kern der eigentliche Funktionsaufruf in die Enclave bildet. Jener verhält sich wie ein Sprung in den Kernel Mode und ist mit entsprechendem Aufwand verbunden. So müssen etwa alte Registerinhalte gesichert und der eintretende logische Prozessor an eine \ac{TCS} gebunden werden. Somit nimmt die reine Prozedur des \ac{ECall} deutlich mehr zeit in Anspruch und hat stets einen höheren Anteil an der Gesamtdauer als dies beim Funktionsaufruf in einer regulären Ausführung der Fall ist. Die Datenrate erreicht letztlich jedoch einen Wert von 680 \ac{MIPS} und liegt somit nur 70 \ac{MIPS} tiefer. Dies ist geringer als der Offset, welcher in der vorigen Untersuchung beobachtet wurde. Die Bedingungen der beiden Untersuchungen sind bezüglich des Kopiervorganges identisch, d.h. die Ausgabebuffer sind jeweils in beiden Fällen so groß wie die Eingabe, wodurch der Kopiervorgang die gleiche Zeit beansprucht. Ausschlaggebend sind jedoch die signifikant geringeren Datenraten des eigentlichen Verarbeitungsschrittes. Folglich nimmt der Kopiervorgang anteilig weniger Zeit am Gesamtprozess in Anspruch. Sobald auch die grüne Kurve den Grenzwert erreicht hat, macht jener quasi den einzigen Unterschied im Vergleich aus der beiden Durchgänge aus.

\paragraph{Einfache Kompressionsverfahren}

An dieser Stelle wird die Verarbeitung durch jeweils eines der in Kapitel 3 betrachteten Kompressionsverfahren, der einfachen Lauflängenkodierung und VByte übernommen. Getestet wurde zunächst jeweils die Dekompression und Kompression. Im Falle von VByte wurde zudem ein kombinierter Ablauf aus beiden Vorgängen untersucht. Auf diese Weise soll der Fall nachgebildet werden, dass die Basisdaten zum Zwecke des Einsparens an Speicherplatz zunächst komprimiert und nach der eigentlichen Verarbeitung wieder dekomprimiert werden, so wie dies im Schema in Abbildung \ref{fig:scenariocomp} betrachtet wurde.

Zum Testen der VByte Kompression wurden zufällig generierte Daten bereitgestellt, deren Größe ein Vielfaches von 4 Byte (32 Bit) aufweist. Dies ist nötig, da VByte in der eingesetzten Form auf entsprechend großen Integern arbeitet. Die Verläufe der Verarbeitungsgeschwindigkeit sind in \ref{fig:eval4} zu sehen. Da es sich bei den eingehenden Daten um zufällige Werte handelt, ist mit Sicherheit davon auszugehen, dass keine guten Kompressionsraten entstanden sind und die Berechnung stets entfernt davon war, im Best Case zu verlaufen. Die Anlaufphase der blauen Kurve ist erwartungsgemäß sehr kurz. Der Grenzwert wird sogar schon nach etwa 300 Byte erreicht. Verantwortlich ist auch hier die höhere Verarbeitungszeit, an welcher der Funktionsaufruf folglich weniger Anteile hat. Auf Seiten der sicheren Verarbeitung jedoch tritt dies erst bei 40.000 Byte ein. Von dort an beträgt der Offset jedoch nur noch geringe 30 \ac{MIPS}, was sich wiederum auf die geringeren Verarbeitungsraten zurückführen lässt. Allerdings besitzt diese Untersuchung das Merkmal, dass der Ausgabebuffer nicht einfach nur gleich groß ist wie jener zur Eingabe. Da VByte im schlechtesten Fall einen 4 Byte Integer durch 5 Byte kodiert, muss für die Ausgabe ein Speicher reserviert werden, welcher 1.25 mal so groß ausfällt. Hierbei zeigt die durch \ac{SGX} zur Verfügung gestellte Sicherheitsmaßnahme des Kopierens seine Schwäche, dass Ein- und Ausgabegrößen vor Aufruf des \ac{ECall} bekannt sein müssen. Somit muss unabhängig von der resultierenden Größe komprimierter Daten der volle Buffer zurückkopiert werden.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval4.pdf}
	\centering
	\caption{Vergleich VByte Kompression}
	\label{fig:eval4}
\end{figure}

Das relativ geringe Verhältnis von Ausgabe- zu Eingabedaten sorgt im Zuge der Komprimierung noch nicht für schwerwiegende Auswirkungen auf das Offset der beiden Durchläufe. Bei einem Blick auf die Dekompression in \ref{fig:eval5} wird dieser Sachverhalt aber schon deutlicher. Im schlechtesten Fall wird hierbei ein einzelnes Byte zu einem ganzen 4 Byte Integer umgewandelt. Somit muss der Ausgabebuffer über die vierfache Größe relativ zur Eingabelänge verfügen. Der zusätzliche Kopieraufwand macht sich bemerkbar, da trotz einer hohen Verarbeitungsgeschwindigkeit der Dekompression, beträgt der Offset etwa 220 \ac{MIPS}. Aufgrund der Eigenschaft des Algorithmus, dass die Datenrate sehr stark von der Form eingehender Daten abhängt, treten mitunter große Schwankungen in der Datenrate auf. Um dem etwas entgegenzuwirken wurden die Zufallsdaten sinnvollerweise vor Ausführung des Benchmarks vorverarbeitet, indem sie durch VByte komprimiert wurden. Dennoch lässt sich in der unsicheren Umgebung ein sehr unruhiger Verlauf erkennen. Im Falle der Enclave ist dies nicht der Fall. Hier besitzt das gleichmäßige Kopieren neben der reinen Verarbeitung einen großen Anteil an der Gesamtdauer des Vorgangs und sorgt für weniger Schwankungen.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval5.pdf}
	\centering
	\caption{Vergleich VByte Dekompression}
	\label{fig:eval5}
\end{figure}

Sowohl die Kompression als auch die Dekompression mittels VByte wurden nochmals in einem Ablauf zusammengefasst. In Abbildung \ref{fig:eval6} sind die entstandenen Verläufe zu sehen. Die eingehenden Daten werden zunächst komprimiert und daraufhin wieder dekomprimiert. Da in diesem vereinfachten Fall als Resultat stets die Eingangsdaten entstehen, ist der Ausgabebuffer ebenso groß wie jene. Im Diagramm kann man daher feststellen, dass die beiden Kurven einen sehr geringen Offset von etwa 20 \ac{MIPS} aufweisen. Dies ist zusätzlich durch die vergleichsweise geringe Verarbeitungsrate von etwas mehr als 350 \ac{MIPS} und der unsicheren Ausführung bedingt. Das Ergebnis zeigt schon einmal deutlich, dass die Berechnung in der Enclave eine durchaus vergleichbare Rate erzielen kann, sofern die durchgeführte Operation komplexerer Natur ist und der ausgehende Kopiervorgang möglichst klein ausfällt.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval6.pdf}
	\centering
	\caption{Vergleich VByte Kompression und Dekompression}
	\label{fig:eval6}
\end{figure}

Die Beobachtungen, welche unter dem Einsatz von VByte und der Iteration getroffen wurden, haben bereits viele Erkenntnisse über das Verhalten der sicheren Domäne in Bezug auf die Datenverarbeitung hervorgebracht. Im Gegensatz dazu weist die Kompression durch ein Lauflängenkodierungsverfahren unterschiedliche Merkmale auf. Der in diesem Kontext interessante Unterschied ist die hohe Variabilität der Kompressionsrate. Sowohl bei der Kompression, aber vor allem bei einer Dekompression ist es durchaus möglich, dass die resultierende Ausgabemenge eine sehr unterschiedliche Länge besitzt.

Ebenso wie bei der Kompression durch VByte besteht die Eingabe der Lauflängenkodierung aus 32 Bit Integerwerten. Der Ausgabebuffer muss doppelt so groß gewählt werden wie der Eingabebuffer. Das liegt daran, dass im ungünstigsten Fall jeder auftretende Wert verschieden seines Vorgängers ist und einen Lauflängenwert von 1 erhält. Jene Kennzeichnungen besitzen im eingesetzten Verfahren ebenso eine Größe von 4 Byte. In Abbildung \ref{fig:eval7} ist entsprechend zu erkennen, dass die Verläufe der Kurven um knapp 160 \ac{MIPS} auseinanderliegen. Die Verarbeitungsraten in der regulären Anwendung ähneln jenen der VByte Dekompression, wodurch die Anlaufphasen der blauen und grünen Kurve ebenso bei jeweils ungefähr 5000 und 25.000 \ac{MIPS} enden. Der Offset ist jedoch geringer, da das Verhältnis der Ausgabe- zur Eingabegröße nur halb so groß ausfällt. Bei etwa 3000 \ac{MIPS} ist ein kleiner Einbruch der Datenrate zu erkennen, welcher durch einen anderen Prozess im Testsystem ausgelöst wurde. Jene Einbrüche sind in der Anlaufphase besonders auffällig, da die dafür verantwortlichen Unterbrechungen bei den anfänglich kleinen Gesamtzeiten einen größeren Einfluss besitzen.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval7.pdf}
	\centering
	\caption{Vergleich Lauflängenkodierung}
	\label{fig:eval7}
\end{figure}

Wie das Ergebnis in Abbildung \ref{fig:eval8} erahnen lässt, bringt die Lauflängendekodierung ein besonderes Problem mit sich. Der große Leistungsunterschied von der regulärer und sicherer Verarbeitung ergibt sich aus der Tatsache, dass der Ausgabebuffer sehr groß gewählt werden muss. Es lässt sich nur sehr schwer abschätzen, wie groß jener ausfallen muss, wenn die Form der zuvor komprimierten Daten nicht bekannt ist. Um keine unrealistisch hohen Lauflängen zuzulassen wurden die zufällig generierten Eingabedaten zusätzlich lauflängenkodiert. Dennoch wurde für die Ausgabe vorsichtigerweise jeweils ein Buffer gewählt, welcher 6-mal so groß ist wie die Eingabe. Als Schrittgröße wurden diesmal 8 Byte festgelegt, da die Dekodierung mindestens ein Paar von Lauflänge und Wert benötigt. Man kann anhand der erreichten Verarbeitungsrate von 2400 \ac{MIPS} gut erkennen, dass das Verfahren grundsätzlich sehr schnell arbeitet. Im Unterschied zu den bisherigen Untersuchungen sorgt dies in Kombination mit dem aufwendigen Kopiervorgang dafür, dass die sichere Berechnung fast nur halb so schnell abläuft. Nach ca. 15.000 Byte erreicht sie die anhaltende Rate von 1100 \ac{MIPS}. Man kann schlussfolgern, dass eine schnelle Verarbeitung durch einen langsamen Kopiervorgang signifikant ausgebremst wird.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval8.pdf}
	\centering
	\caption{Vergleich Lauflängendekodierung}
	\label{fig:eval8}
\end{figure}

\section{Kryptographische Verarbeitung}

Die dritte und letzte Untersuchungsreihe beschäftigt sich mit der Einbeziehung kryptographischer Operationen in den Verarbeitungsprozess. Jene kommen zum Einsatz, um die zunächst verschlüsselten Basisdaten in der Enclave verarbeiten zu können. Hierzu muss entsprechend die Annahme getroffen werden, dass die benötigten Schlüssel in der Enclave gespeichert sind. Es war bereits in den vorigen Ergebnissen klar zu erkennen, dass das Kopieren der Daten vor und nach der sicheren Verarbeitung mit einer großen Leistungseinbuße einherging. Im Zuge der folgenden Untersuchungen wird jener Vorgang stets durch Angabe des user\textunderscore check Attributes weggelassen. Auf diese Weise kann nun geschaut werden, inwiefern sich die getesteten Verfahren unter Hinzunahme der kryptographischen Funktionen verhalten und welcher Leistungsunterschied noch zur regulären Durchführung verbleibt. Der verglichene Ablauf unterscheidet sich nur noch durch die Hinzunahme der Enclave als Einheit zwischen Ein- und Ausgabe, wodurch sich das Datenflussschema in Abbildung \ref{fig:settingsection3} ergibt. Augenmerk der ersten Untersuchung ist nochmals das einfache Ent- und Verschlüsseln der Daten, so wie dies bereits auf die Auswirkung des Kopiervorgangs hin der Fall war. Diesmal jedoch wird der Prozess ohne Kopieren mit der unsicheren Ausführung verglichen. Der zweite Teil dreht sich um die Kombination der kryptographischen Zwischenschritte mit dem Kompressionsverfahren VByte. Zuletzt werden die zuvor getätigten Tests in einem umfangreichen Gesamtablauf zusammengefasst.

\begin{figure}[h]
	\includegraphics[width=0.7\linewidth]{img/SettingSection3.pdf}
	\centering
	\caption{Untersuchungsschemata der kryptographischen Verarbeitung}
	\label{fig:settingsection3}
\end{figure}

\paragraph{Ent- und Verschlüsselung}

Die Bedingungen für diesen Test sind die gleichen wie bei jenem innerhalb der Reihe enclaveinterner Untersuchungen. Der eigentliche Verarbeitungsschritt wird ausgelassen und die Ausgabedatenmenge ist stets leer. Als Schrittgröße werden in diesem und den kommenden Untersuchungen aufgrund der eingangs abgeschlossenen Entschlüsselung stets 16 Byte verwendet. Abbildung \ref{fig:eval9} veranschaulicht das entstandene Resultat.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval9.pdf}
	\centering
	\caption{Vergleich Entschlüsselung und Verschlüsselung (ohne Kopieren)}
	\label{fig:eval9}
\end{figure}

Wie zu erwarten, erreichen beide Durchläufe eine ähnlich hohe Verarbeitungsrate. Nachdem die Anlaufphase aufgrund des aufwendigeren \ac{ECall} in der sicheren Anwendung entsprechend länger ausfällt, wird nach etwa 32.000 Byte ein Wert von 1100 \ac{MIPS} erreicht. Dieser liegt somit nur sehr knapp unter den fast 1200 \ac{MIPS} des regulären Durchgangs. Es ist zu erwarten, dass sich die beiden Kurven im weiteren Verlauf weiter annähern, da der konstante Anteil im Verhältnis zur Eingabemenge immer geringer wird. Dies ist auch deutlich im Differenzgraphen zu erkennen.

\paragraph{Verschlüsselung und Kompression}

Nachfolgend wird die kryptographische Verarbeitung mit der Anwendung von VByte kombiniert. Während in der vorhergehenden Untersuchung die Form der verschlüsselten Basisdaten nicht von Relevanz war, wird nun die Annahme getroffen, dass diese in einer komprimierten Form verschlüsselt wurden. Der Gesamtablauf wurde zur Untersuchung in zwei Teilabläufe unterteilt, um ein unabhängiges Testen der Kompression und Dekompression zu ermöglichen. Dies kann in Abbildung \ref{fig:settingeval10} nachverfolgt werden. Links ist der erste Test zu sehen, in welchem die eingehenden Daten entschlüsselt und daraufhin dekomprimiert werden. Im zweiten Ablauf liegen die Daten schon in ihrer richtigen Form vor, werden also wieder komprimiert und entschlüsselt. Das Ergebnis des linken Teilablaufs ist in Abbildung \ref{fig:eval10} und jenes des rechten in Abbildung \ref{fig:eval11} zu sehen.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/SettingEval10.pdf}
	\centering
	\caption{Untersuchungsschemata Verschlüsselung und Kompression}
	\label{fig:settingeval10}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval10.pdf}
	\centering
	\caption{Vergleich Entschlüsselung und Dekomprimierung}
	\label{fig:eval10}
\end{figure}

Die ungesicherte Entschlüsselung und Dekompression erreicht nach etwa 18.000 Byte eine Rate von 820 \ac{MIPS}. Unter Nutzung der Enclave wurden etwa 70.000 Byte benötigt, bis der Durchlauf am ungefähren Höchstwert von 810 \ac{MIPS} angekommen ist. Beide Kurven besitzen fortan also einen vernachlässigbaren Offset. Außerdem sind gerade ab einer Eingabelänge von 250.000 Byte einige Schwankungen in den Verläufen zu beobachten. Wie auch schon bei der vorherigen Untersuchung von VByte wurden die Daten zuvor vorbereitet. Neben einer Komprimierung der zufälligen Daten fand auch eine Verschlüsselung (unter den gleichen kryptographischen Parametern) statt. Dennoch scheint die hohe Entropie in der Form von den vorliegenden Daten für gewisse Schwankungen zu sorgen.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval11.pdf}
	\centering
	\caption{Vergleich Komprimierung und Verschlüsselung}
	\label{fig:eval11}
\end{figure}

Jene Schwankungen werden in der Komprimierung und Verschlüsselung noch viel deutlicher. Während die Entschlüsselung zuvor auch dank einer konstanten Einhaltung der Blockgröße stets Eingabedaten auf gleich große Ausgaben abbildete, so sieht dies in jenem zweiten Teilablauf anders aus. Je nachdem wie hoch die Kompressionsrate ausfällt, so werden Abschnitte unterschiedlicher Länge verschlüsselt. Dies hat einen sehr unruhigen Verlauf der beiden Kurven zur Folge. Ansonsten ähnelt dieser sehr dem Ergebnis des ersten Teilablaufs. Da die Kompression wie auch schon zuvor gesehen langsamer verläuft als die Dekompression, sind die Anlaufphasen in geringem Maße nach links verschoben. Schon nach etwa 10.000 Byte werden die knapp 500 \ac{MIPS} im regulären Durchgang erreicht. In etwa der gleiche Wert tritt ab 30.000 Byte im grünen Verlauf auf. Aufgrund der hohen Schwankungen kann kein großer Leistungsunterschied zwischen den Berechnungen ausgemacht werden. Der Einsatz von \ac{SGX} kann insgesamt in beiden der untersuchten Teilabläufe sehr gut mithalten.

\paragraph{Gesamtablauf}

Zum Abschluss der Untersuchungsreihe wird ein Ablauf betrachtet, welcher die zuletzt betrachteten Teilabläufe kombiniert und einen zusätzlichen Zwischenschritt hinzufügt. Der gesamte Ablauf kann in \ref{fig:settingeval12} nachvollzogen werden. Nach dem Entschlüsseln und Dekomprimieren der eingehenden, durch VByte komprimieren Daten übernimmt exemplarisch eine Lauflängenkodierung die eigentliche Verarbeitung. Somit werden die Daten anderweitig komprimiert, um sie beispielsweise in späteren Zwischenschritten zu aggregieren, aber trotzdem Platz zu sparen. Daraufhin wird wie zuvor eine Komprimierung durch VByte vorgenommen, gefolgt von der Verschlüsselung als letzter Teilschritt zum Endergebnis. Dieser letzte Versuch kombiniert also viele der zuvor untersuchten Operationen, während weiterhin auf ein Kopieren verzichtet wird. Es hat sich bereits gezeigt, dass sich der Kopiervorgang sehr nachteilig auf die Leistung der gesicherten Ausführung eines Ablaufs auswirkt. Ziel dieser Untersuchung ist es jedoch zu sehen, ob die komplexere Verarbeitung unter \ac{SGX} eine vergleichbare Geschwindigkeit erreichen kann. Abbildung \ref{fig:eval12} zeigt das entstandene Ergebnis.

\begin{figure}[h]
	\includegraphics[width=0.7\linewidth]{img/SettingEval12.pdf}
	\centering
	\caption{Untersuchungsschema Gesamtablauf}
	\label{fig:settingeval12}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval12.pdf}
	\centering
	\caption{Vergleich Gesamtablauf}
	\label{fig:eval12}
\end{figure}

Entsprechend der Anzahl kombinierter Operationen ergibt sich erwartungsgemäß eine viel kleinere Verarbeitungsrate als in den bisherigen Tests. Dies hat sichtbare Auswirkungen auf die Länge der Anlaufphasen. Im Falle der blauen Kurve dauert sie etwa 7000 Bytes an. Dann ist der ungefähre Endwert von 200 \ac{MIPS} erreicht. Im Verlauf unter \ac{SGX} werden mindestens etwa 100.000 Byte nötig bis der Wert gehalten wird. Dies wäre wohl schon eher der Fall, wäre nicht ein Einbruch der Kurve bei ca. 69.000 Byte. Tendenziell liegt der erreichte Wert über den gesamten Verlauf gesehen unwesentlich unter dem der unsicheren Verarbeitung. Während dies in der Theorie durchaus stimmt, so machen es bereits die kleinen auftretenden Schwankungen unmöglich einen wirklichen Unterschied in den Verläufen zu erkennen. Gerade angesichts der Menge an Teilschritten, welche potenziell eine große Entropie zur Folge hat, sind die Ergebniswerte vergleichsweise stabil. Der Grund ist auch hier die Vorverarbeitung der Zufallsdaten durch Komprimierung und Verschlüsselung. An manchen Stellen sind jedoch größere Einstürze der Rate zu verzeichnen. In der grünen Kurve ist das neben der bereits angesprochenen Stelle auch bei etwa 136.000 Byte der Fall. Nennenswert ist in der blauen Kurve ein Tief gegen Ende des Durchlaufs kurz vor 400.000 Byte. Die Ursachen liegen allen Anschein nach, außerhalb der Testdurchführung, im Testsystem.

\section{Fazit}

Die insgesamt 12 Untersuchungen haben viele Erkenntnisse über das Verhalten einer \ac{SGX} Anwendung in einem datenverarbeitenden Kontext offenbart. Zunächst haben sich jene auf die Charakteristika der Enclave selbst beschränkt. Im Mittelpunkt stand die Möglichkeit, das standardmäßig durchgesetzte Kopieren der Ein- und Ausgabe auszuschalten. Es hat sich gezeigt, dass dies die Verarbeitungsrate in signifikantem Umfang erhöht. Dies wurde im zweiten Teil, dem Einsatz der Kompressionsverfahren als Verarbeitungsoperationen, bestätigt. Lässt man diesen Aufwand außer Acht, verbleibt ein mit zunehmender Datenmenge vernachlässigbarer Mehraufwand des \acp{ECall} gegenüber einem herkömmlichen Funktionsaufruf. Aufgrund eben jenen Kopierens lassen sich die Leistungsunterschiede zwischen den unterschiedlichen, getesteten Operationen zusammenfassend auf zwei Gründe zurückführen:

\paragraph{Größe der zu kopierenden Buffer}
Da die Verarbeitungsgeschwindigkeit im Verhältnis zur Eingabemenge betrachtet wird, lässt sich dieser Punkt auf die Größe des Ausgabebuffers, bzw. des Verhältnisses von Ausgabe- zu Eingabelänge reduzieren. Je kleiner diese ausfällt, desto geringer der Offset zur unsicheren Verarbeitung.

\paragraph{Tatsächliche Verarbeitungsgeschwindigkeit}
Der durch das Kopieren entstehende Offset fällt verhältnismäßig kleiner aus, je geringer die reine Datenrate des eingesetzten Verfahrens ist. Die Längen der Anlaufphasen liegen somit außerdem näher beieinander, so dass ein konstanter Offset der beiden verglichenen Ausführungen eher auftritt.

Die letzte Reihe an Untersuchungen nahm den Kopiervorgang ganz außer Acht. Diesmal wurden die Kompressionsverfahren mit kryptographischen Operationen in Verbindung gebracht, da dies der praktischen Einsatzform entsprechen würde. In allen Tests hat sich klar gezeigt, dass die Geschwindigkeiten auf Seiten der Enclave nahezu identische Werte erreichten wie in einer herkömmlichen Anwendung. Auch in einem umfangreichen Ablauf sind nur marginale Unterschiede zwischen den Datenraten zu verzeichnen. Diese ergeben sich vor allem dadurch, dass der \ac{ECall} bei kleineren Eingabelängen für einen größeren Offset sorgt, also die Anlaufphase stets etwas länger andauert als in der unsicheren Ausführung.

Aufgrund der erhaltenen Erkenntnisse lassen sich einige Schlussfolgerungen für den praktischen Einsatz von \ac{SGX} in einem datenverarbeitenden System treffen. Aufgrund des zusätzlichen Aufwandes sollten \acp{ECall} stets auf möglichst großen Datenmengen stattfinden anstatt auf vielen kleineren. So ist gar eine Verarbeitung von einzelnen Eingabewerten alles andere als sinnvoll und sollte von vorn herein durch das in den Tests eingesetzte Modell des Bulk Processing ersetzt werden. Insofern sorgfältige Programmierpraktiken und Evaluationsvorgänge des Codes in der Enclave von statten gehen, ist es sehr empfehlenswert, die standardmäßigen Sicherheitsvorkehrungen der Edge Routines, bzw. das Kopieren der Buffer abzuschalten. Werden diese Vorkehrungen getroffen, so ist eine leistungsfähige Datenverarbeitung in der Enclave gewährleistet.