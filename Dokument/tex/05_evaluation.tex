%TODO Möglicherweise Vorkommen von "Schemata" durch "Datenflussschemata" ersetzen
\chapter{Evaluierung}

Wie bereits in den vorangegangenen Kapiteln betrachtet wurde, verspricht Intel SGX das Aufweisen guter Sicherheitsmerkmale und eine leichte Systemintegration. Obwohl der herkömmliche Einsatzzweck eher auf die reine Speicherung sensibler Daten beruht, ist es das Ziel dieser Arbeit zu untersuchen, ob es sinnvoll ist, Intel SGX im Zuge der Anfrageverarbeitung in einem hauptspeicherbasierten Datenbanksystem einzusetzen. In diesem Kapitel erfolgt eine Bewertung dieses Anwendungsfalls anhand von Leistungsbenchmarks verschiedener Szenarien der Datenverarbeitung.

Zunächst wird hierzu eine Einführung in das Konzept der Untersuchungen gegeben. Die darauffolgenden Abschnitte befassen sich jeweils mit den konkreten Punkten des Testplans, wobei die erfassten Ergebnisse gezeigt und Beobachtungen getroffen werden. In einem abschließenden Fazit erfolgt eine Auswertung jener Ergebnisse.

\section{Konzept}

Der folgende Abschnitt befasst sich mit dem konzeptionellen Vorgehen bei der Durchführung der einzelnen Untersuchungen. Es wird zunächst der Frage nachgegangen, welche Tests in dem vorliegenden Kontext sinnvoll sind. Somit liegt ein konkreter Testplan vor, dessen Ergebnisse im Laufe des Kapitels sukzessive beschrieben werden können. Im zweiten Abschnitt wird die eingesetzte Testumgebung aus Sicht von Soft- und Hardware beschrieben. Hierbei wird vor allem kurz auf wichtige Herangehensweisen in der Implementierung eingegangen.

\subsection{Untersuchungen}

Die Datenverarbeitung innerhalb der Enclave wird im Folgenden durch das allgemeine Schema des Datenflusses in Abbildung \ref{fig:scenarios} modelliert.
\begin{figure}[h]
	\includegraphics[width=0.9\linewidth]{img/EvalScenarios.pdf}
	\centering
	\caption{Allgemeines Szenario der Datenverarbeitung}
	\label{fig:scenarios}
\end{figure} 
Es beinhaltet die einzelnen Teilschritte, welche zu einem vollständigen Berechnungsprozess gehören, beginnend mit dem Transfer von gespeicherten und Basisdaten in die sichere Komponente der Enclave. In unserem Kontext liegt ein nicht vertrauenswürdiges Umfeld vor. Es wird daher die Annahme getroffen, dass jene Basisdaten stets in einer verschlüsselten Form vorliegen. Die Zwischenschritte ergeben sich dann wie folgt:

\paragraph{Kopieren}
Die Basisdaten werden zu Beginn in die Enclave transferiert. Auf technischer Ebene erfolgt dies durch ein Kopieren des Buffers in den gesicherten Speicher. Zum Ende der Verarbeitung erfolgt ein entsprechendes Zurückkopieren des Endergebnisses in die unsichere Domäne.

\paragraph{Entschlüsseln/Verschlüsseln}
Sobald sich die Daten in der Enclave befinden, müssen sie zunächst unter Hinzunahme eines geheimen, gespeicherten Schlüsseln entschlüsselt werden, um eine Verarbeitung zu ermöglichen. Nachdem die Arbeit auf den Daten beendet wurde, wird wieder eine Verschlüsselung vorgenommen, um die ausgehenden Daten zu schützen.

\paragraph{Verarbeiten}
Die eigentliche Verarbeitung findet auf den Klartextdaten statt und beinhaltet eine einzelne oder Abfolge von Operationen. Im zweiten Fall entstehen unverschlüsselte Zwischenergebnisse.

\paragraph{}
Die Schritte vor und nach der eigentlichen Verarbeitung sind offensichtlich mit einem gewissen rechnerischen Aufwand verbunden. Von daher ist es erstrebenswert, möglichst viele der zu verarbeitenden Daten auf einmal zu Kopieren und in der Enclave zu halten. Demgegenüber steht allerdings der stark eingeschränkte Speicherplatz, welcher im PRM zur Verfügung steht. Ein Weg, dem entgegenzuwirken ist der Einsatz von den in Kapitel 3 beschriebenen Kompressionsverfahren. Sie ermöglichen eine Reduzierung der Datenmenge im sicheren Bereich auf Kosten eines weiteren Berechnungsschrittes je für die Komprimierung und Dekomprimierung. Nach Zunahme dieser Schritte ergibt sich das Schema in Abbildung \ref{fig:scenariocomp} Je nach Art der eigentlichen Berechnung ist es mitunter möglich, direkt auf den komprimierten Daten zu arbeiten. Dazu wurde beispielsweise die Bildung des Summenaggregats auf einer lauflängenkodierten Menge betrachtet. Sollte dies nicht der Fall sein, beinhaltet die Menge an Operationen im Verarbeitungsschritt weitere (De)komprimierungsschritte.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/EvalScenariosComp.pdf}
	\centering
	\caption{Szenario der Datenverarbeitung mit (De)komprimierung}
	\label{fig:scenariocomp}
\end{figure}

Ebenso denkbar ist, dass die Basisdaten bereits in einer komprimierten Form vorliegen, wie dies in einer In-Memory Datenbank durchaus üblich ist. In diesem Fall ist ein Kompressionsschritt vor der Verarbeitung nicht obligatorisch. Stattdessen kann sogar erst eine Dekomprimierung stattfinden, sollte es nicht anders möglich sein die Berechnungen durchzuführen. Denkbar ist dann auch eine Kompression unter Nutzung eines anderen Verfahrens oder eine Doppelte. Ein Vorteil ist dadurch gegeben, dass kleinere Datenmengen in die Enclave kopiert werden müssen, was außerdem in einer schnelleren Entschlüsselung resultiert. Nach der Verarbeitung kann es durchaus möglich sein, dass die Daten auch weiterhin komprimiert bleiben. Üblicherweise findet hier jedoch wieder eine Dekomprimierung der Ergebnisdaten in die ursprüngliche Repräsentation statt, bevor es zur Verschlüsselung und dem Kopieren geht.

Der Vollständigkeit wegen muss an dieser Stelle auch angemerkt werden, dass eigenschaftserhaltende Verschlüsselungsverfahren ihren Einsatz finden können, um einen Teil des Aufwandes durch Ver- bzw. Entschlüsselungen einzusparen. Ein mögliches Szenario wäre das Auftreten von Speicherknappheit in der Enclave, wodurch ein Teil der Daten zunächst ausgelagert werden müsste. Da ohnehin eine vorausgehende Verschlüsselung notwendig ist, kann beispielsweise ein deterministisches Verschlüsselungsschema angewandt werden. Dies erlaubt eine parallele Bearbeitung außerhalb der Enclave, bevor es zu einem erneuten Transfer einschließlich Kopiervorgang und Verschlüsselung kommt. Im Falle des deterministischen Verfahrens wäre es etwa möglich, Gleichheitsprädikate zu prüfen und einen Join zwischen zwei Spalten durchzuführen. 

Es wird klar, dass es durchaus eine große Fülle von weiteren Szenarien geben kann. In Bezug auf die Untersuchungen wird sich daher auf die geläufigsten beschränkt. Bei Betrachtung der beiden aufgeführten Datenflussschemata sind stets zwei Kopiervorgänge am Gesamtablauf beteiligt. In Kapitel 4 wurden diese als Sicherheitsmaßnahme vorgestellt, welche durch die Edge Routines realisiert wird. Mit dem Setzen des user\textunderscore check Attributs an den Funktionsargumenten wurde zudem ein Mechanismus vorgestellt, welcher ein direktes Arbeiten auf den Daten ohne Kopieren ermöglicht. Demzufolge ergibt sich das in Abbildung \ref{fig:scenarionocopy} gezeigte Schema. Beide Varianten werden zunächst ohne einen konkreten Verarbeitungsschritt verglichen. Die beiden hierzu getätigten Tests werden als enclaveinterne Untersuchungen zusammengefasst. Das Ziel ist es, herauszufinden, wie groß der durch das Kopieren der Buffer entstehende Overhead ist.

\begin{figure}[h]
	\includegraphics[width=0.7\linewidth]{img/EvalScenariosNoCopy.pdf}
	\centering
	\caption{Szenario der Datenverarbeitung ohne Kopiervorgänge}
	\label{fig:scenarionocopy}
\end{figure}
%TODO Erwähnen: Iteration als ganz grundlegende Operation
Die darauffolgende Reihe an Tests beschäftigt sich mit dem Aspekt der konkreten Datenverarbeitung. Es wird dabei jeweils ein Ablauf in der Enclave mit einer gleichen Abfolge innerhalb eines regulären Programms verglichen. Anstelle des Verarbeitungsschrittes werden Operationen aus den folgenden Bereichen eingesetzt:

\begin{itemize}
	\item Allgemeine Datenverarbeitung
	\begin{itemize}
		\item Iteration über die Datenmenge
	\end{itemize}
	\item Einfache Kompressionsverfahren
	\begin{itemize}
		\item VByte (De)kompression
		\item Lauflängen(de)kodierung
	\end{itemize}
\end{itemize}

Der Kernpunkt dieser Untersuchungen ist herauszufinden, ob die Leistung signifikant durch die sicheren Berechnungen beeinflusst wird. Interessant ist auch, ob die einzelnen Verfahren gewisse Merkmale aufweisen, welche eine Verarbeitung mittels SGX zusätzlich erschweren. Die Zwischenschritte zur Ent- und Verschlüsselung werden zunächst außer Acht gelassen, um einen besseren Eindruck über die jeweilige Operation und das Zusammenspiel mit dem im Falle der Enclave auftretenden Kopiervorgang zu bekommen.

Die dritte Menge von Untersuchungen widmet sich hingegen einer Einbeziehung der kryptographischen Funktionalität. Im Vordergrund steht dessen Kombination mit einer verarbeitenden Operation. Um zu testen, wie die Leistung der kryptographischen Algorithmen abschneidet, wird als erstes das in Abbildung \ref{fig:scenarionocopy} aufgegriffen, in einem herkömmlichen Umfeld durchgeführt und mit dem Verlauf in der Enclave verglichen. Daraufhin wird jeweils die Kompression durch VByte mit der Ent- und Verschlüsselung kombiniert. Das Kopieren wird in allen drei Fällen außer Acht gelassen.

Um ein Gesamtbild von der Leistung unter SGX zu erhalten, wird abschließend ein komplexer Vorgang getestet. Das Ziel ist es herauszufinden, wie gut die Technologie in einer längeren Berechnung abschneidet, während die Bedingungen gegenüber einer regulären Durchführung möglichst identisch sind. Daher findet auch hier kein Kopiervorgang statt.
%TODO Mitunter ein paar der oberen Details in die einzelnen Beschreibungen der Settings auslagern
\subsection{Testumgebung}
%TODO Es wurden Mittelwerte gebildet mit Fenstergröße 50
Die Umsetzung der Testumgebung erfolgte mit dem Ziel, die einheitliche Durchführung jeglicher Untersuchungen zu erlauben. Wichtig war von daher eine genaue Festlegung des zu bewertenden Sachverhalts. Da es sich bei sämtlichen Tests um Messungen in Bezug auf die Leistung (Performance Benchmarks) handelt, welche im Kontext der Datenverarbeitung stattfindet, wurde das gängige Maß MIPS (Million Instructions per Second) gewählt. In diesem Fall orientiert es sich aber nicht auf die durchgeführten Prozessorinstruktionen, sondern misst verarbeitete Byte, genauer gesagt Megabyte je Sekunde. 

Auf technischer Ebene ist jeder zu testende Ablauf durch eine Funktion realisiert, weshalb beide Begriffe nachfolgend synonym gebraucht werden. Während des Testdurchgangs wird der jeweilige Ablauf auf einer der Größe nach kontinuierlich steigenden Menge an Eingabedaten ausgeführt. Alle getesteten Funktionen sind daher, wie in der Praxis üblich in der Lage, variable Datenmengen entgegenzunehmen und zu verarbeiten (Bulk Processing). Es ergibt sich ein Verlauf der gemessenen MIPS je Größe des Eingabebuffers. Die unterschiedliche Natur der Funktionen bedingt jedoch, dass es keine einheitliche Schrittfolge geben kann, nach welcher sich die Eingabegröße erhöht. Kompressionen durch VByte und Lauflängenkodierung erfordern etwa 32 Bit (4 Byte) Integer Werte als Eingabe, während die Durchführung einer einfachen Iteration byteweise erfolgen kann. Um dennoch eine kontinuierliche Steigung der Eingabemenge zu ermöglichen, wurde die Schrittfolge entsprechend für jeden Test angepasst, während die Anzahl unterschiedlicher Ausprägungen von Eingabewerten einer konkreten Festlegung entspricht. Gegeben ist eine Funktion mit konstanter Schrittlänge $s$ in Byte. Die Größe der Eingabedaten $d$ (Bytes) im Schritt $i$ ergibt sich wie folgt: 

\begin{equation*}
	d = i * s; i \in [0, 25000]
\end{equation*}

Um Schwankungen in den Berechnungen, etwa aufgrund des Schedulings entgegenzuwirken, wurde jeweils pro Ausprägung des Eingabewertes ein Mittelwert der MIPS aus 200 Ausführungen gebildet. Der Eingabebuffer wird vor deren Ausführung mit Testdaten gefüllt, welche je nach Testfall von unterschiedlicher Form sind.

Die beschriebene Methodik zur Leistungsanalyse wurde in eine gesonderte Bibliothek ausgelagert. Zum Zwecke der Untersuchungen wurden zudem zwei unabhängige C++ Anwendungen erstellt. Erstere bindet eine Enclave ein, in welcher die zu testenden Operationen umgesetzt sind. Über jene Anwendung erfolgen die Messungen der sicheren Domäne. Die zweite ist ein Gegenstück ohne die Nutzung von SGX. In diesem Fall sind die Abläufe auf gewöhnliche Art implementiert. Die zum Einsatz gekommenen Kompressionsalgorithmen wurden zum Zwecke der Untersuchungen zur Verfügung gestellt. Beide Anwendungen nutzten dahingegen die \textit{Intel Integrated Performance Primitives Cryptography} Bibliothek zur Einbettung der benötigten kryptographischen Algorithmen. Konkret kam in allen entsprechenden Tests eine Implementierung von AES CBC mit der Schlüssellänge 128 Bit zum Einsatz. Kompiliert wurden beide Programme mit dem Intel C++ Compiler in der Version 17. Jegliche Compileroptimierungen wurden jeweils zur Gewährleistung eines fairen und sauberen Testergebnisses deaktiviert. 

Alle Untersuchungen fanden unter Windows 10 auf der gleichen Hardware statt. Das System verfügte über 16 GB RAM und als Prozessor diente ein Intel Core i5-7600 (Kaby Lake) mit 3,50 GHz Taktfrequenz.

%TODO Achtung! Jeweils darauf eingehen, welche Art von Daten als Eingabe vorliegt
\section{Enclaveinterne Untersuchungen}

Die erste Reihe an Untersuchungen ist zunächst sehr grundlegend und vergleichsweise einfach aufgebaut. Sie beschäftigt sich mit dem Vergleich der Ansätze zur Vor- und Nachbereitung der eigentlichen Datenverarbeitung in einer Enclave. Im Vordergrund steht das durch die Edge Routines durchgesetzte Kopieren der Ein- und Ausgabedaten zwischen der Enclave und unsicheren Anwendung, welche ihre Dienste beansprucht. Verglichen wird dies mit einem Vorgehen, in welchem das Kopieren durch Setzen des user\textunderscore check Attributes in der Enclavedefinition entfällt. Wie bereits im vorangegangenen Kapitel beschrieben, ist jene Maßnahme nur mit äußerster Vorsicht zu treffen.

\paragraph{Kopieren der Daten in und aus Enclave}

Grundlage der ersten Untersuchung sind die beiden in Abbildung \ref{fig:settingeval1} dargestellten Schemata. Auf der linken Seite ist der Verlauf dargestellt, der ein Kopieren von Eingabedaten in die Enclave beinhaltet. Der eigentliche Funktionsaufruf enthält keinerlei Verarbeitungen und kehrt sofort zurück, was ein abschließendes Kopieren der leeren Ausgabedaten zur Folge hat. Auf diese Weise kann der Kopiervorgang unabhängig von der eigentlichen, in der Enclave durchzuführenden Funktionalität untersucht werden. Die Größe des Ausgabebuffers ist jeweils stets gleich jener der Eingabe. Das Schema auf der rechten Seite sieht keinen Kopiervorgang vor, womit nur ein einfacher ECALL verbleibt. Obwohl dieser ebenso durch die Edge Routines realisiert wird, so übernehmen diese keinerlei sonstige Berechnungen. Die vorliegende Form der Daten ist für diese Tests vollkommen ohne Relevanz, weshalb etwa eine Verschlüsselung nicht berücksichtigt wurde. Das Ergebnis der beiden Durchgänge ist in Abbildung \ref{fig:eval1} zu sehen.

\begin{figure}[h]
	\includegraphics[width=0.9\linewidth]{img/SettingEval1.pdf}
	\centering
	\caption{Datenflussschemata der ersten Untersuchung}
	\label{fig:settingeval1}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval1.pdf}
	\centering
	\caption{Vergleich Kopieren der Daten in die Enclave und kein Kopieren mittels user\textunderscore check}
	\label{fig:eval1}
\end{figure}

Erwartungsgemäß verläuft die blaue Kurve konstant linear steigend. Dies ist damit zu begründen, dass  gar keine Abhängigkeit von der eigentlichen Eingabemenge besteht. Der einzige limitierende Faktor ist ein konstanter Overhead durch den ECALL und des damit verbundenen Kontrollflusses durch die Edge Routines. Je größer die Eingabedaten werden, desto höher ist folglich die Zahl an MIPS. Die grüne Kurve verzeichnet einen Verlauf, dessen Steigung mit zunehmender Anzahl Bytes gleichmäßig abnimmt und sich einem Grenzwert von etwa 6000 MIPS annähert. Somit bleibt sie im Vergleich immer weiter zurück und die Steigung der Differenzkurve nähert sich zunehmend jener der Blauen. Dieser Verlauf kann als eine Art Anlaufphase betrachtet werden. Idealerweise würde die Kurve von Beginn an einen konstanten Wert aufweisen, da die in den Edge Routines ausgeführten Operationen wie das Kopieren der Buffer idealerweise eine lineare Komplexität aufweisen. Entgegen wirkt jedoch der konstante Aufwand, welcher durch den Aufruf des ECALLs entsteht, etwa durch die damit verbundenen Überprüfungen, bzw. Funktionsaufrufe an sich. Jener Aufwand ist im Verhältnis zur eigentlichen, linearen Verarbeitung durch das Kopieren anfangs noch sehr groß. Mit zunehmender Eingabemenge fällt der konstante Anteil an der Berechnungszeit jedoch immer kleiner aus und wird vernachlässigbar, wodurch sich der Prozess seiner tatsächlichen Verarbeitungsrate von ca. 6000 MIPS nähert.

\paragraph{Kopieren sowie Ent- und Verschlüsselung}

Während die vorige Untersuchung lediglich ein Kopieren auf beliebigen Daten beinhaltete, so wird im Folgenden davon ausgegangen, dass diese zusätzlich eine verschlüsselte Form aufweisen. Das Modell wird daher um je einen Schritt zur Entschlüsselung nach dem Eintritt in die Enclave sowie einen Verschlüsselungsschritt vor dem Verlassen ergänzt. Im Mittelpunkt steht auch weiterhin ein Vergleich der Verarbeitungsgeschwindigkeit des Ablaufs mit und ohne Kopieren. Somit ergeben sich die in Abbildung \ref{fig:settingeval2} gezeigten Schemata.
% TODO Oben rein: Es wurde eine Schrittgröße von 16 Byte gewählt (128 Bit), da dies der Blockgröße des eingesetzten AES Verschlüsselungsverfahrens entspricht. Auf diese Weise steigt der Aufwand je Ausprägung von Eingabewert definitiv an. 

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/SettingEval2.pdf}
	\centering
	\caption{Datenflussschemata der zweiten Untersuchung}
	\label{fig:settingeval2}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval2.pdf}
	\centering
	\caption{Vergleich Ent- und Verschlüsselung mit und ohne Kopiervorgang}
	\label{fig:eval2}
\end{figure}

Das in Abbildung \ref{fig:eval2} gezeigte Ergebnis unterscheidet sich entsprechend der zusätzlichen Berechnungen von dem der ersten Untersuchung. Das hängt damit zusammen, dass nun in beiden Durchgängen eine echte Relation zwischen Datenmenge und Berechnungsaufwand existiert. Entsprechend des Verschlüsselungsverfahrens bzw. Operationsmodus ist diese wie erwartet linearer Natur. Es kann auch hier die Beobachtung getroffen werden, dass eine Anlaufphase auftritt, bevor der konstante Wert erreicht wird. Da der dafür verantwortliche ECALL jedoch in beiden Fällen etwa den gleichen konstant andauernden Anteil aufweist, dauert jene Phase jeweils bis zu der gleichen Datenmenge von ca. 100.000 Byte an. Im Differenzgraphen lässt sich ein gleichmäßiger Offset von ungefähr 100 MIPS beobachten, welcher durch die Kopiervorgänge und Überprüfungen im linken Ablauf des Untersuchungsschemas entsteht.

\section{Einfache Datenverarbeitung}

Die nun folgenden Untersuchungen befassen sich mit verschiedenen datenverarbeitenden Verfahren. Jene werden zum einen innerhalb der Enclave und außerdem in einer regulären Anwendung durchgeführt. Der Vergleich beider Ergebnisse liefert jeweils eine Aussage über die zu erwartende Leistung des Verfahrens, sollte dessen Ausführung in eine gesicherte Umgebung ausgelagert werden. Allgemein lassen sich die getroffenen Untersuchungen durch die in Abbildung \ref{fig:settingsection2} gezeigten Schemata zusammenfassen. Links ist der jeweilige Durchgang dargestellt, welcher eine Enclave einbezieht. Die ein- und ausgehenden Daten werden stets wie üblich kopiert. Um den Schwerpunkt auf die Verarbeitungsverfahren zu legen, entfallen jegliche kryptographische Operationen. Im zentralen Zwischenschritt wird je Untersuchung ein konkretes Verfahren eingesetzt. Auf der rechten Seite geschieht der Ablauf durch die Ausführung des Verfahrens auf einem gegebenen Buffer zur Eingabe und ein Ablegen des Resultats in einem entsprechenden Ausgabebuffer.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/SettingSection2.pdf}
	\centering
	\caption{Untersuchungsschemata der einfachen Datenverarbeitung}
	\label{fig:settingsection2}
\end{figure}

\paragraph{Iteration über Datenmenge}

Als erstes wird mit der Iteration über die Eingabemenge ein Verfahren untersucht, welches in der Datenverarbeitung sehr gängig ist. Seinen Einsatz findet es etwa bei der Aggregation von Spalten zur Summen- oder Durchschnittsbildung, aber beispielsweise auch bei der Prüfung von Prädikaten im Zuge eines Joins zweier Spalten. Entsprechend bildet es stets nur einen grundlegenden Bestandteil einer komplexeren Operation. Für den Test wird die komplette Eingabedatenmenge byteweise iteriert, während der aktuelle Wert einer Variablen zugewiesen wird. Somit erfolgt eine Erhöhung der Datenmenge um ein Byte je Schritt. Als Ausgabe dient lediglich ein leerer Buffer. Abbildung \ref{fig:eval3} zeigt das entstandene Ergebnis.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval3.pdf}
	\centering
	\caption{Vergleich Iteration über Datenmengen}
	\label{fig:eval3}
\end{figure}

Es ist diesmal gut zu erkennen, dass die Anlaufphasen der beiden Kurven sehr unterschiedlich verlaufen. Die Berechnung außerhalb einer Enclave erreicht schon sehr zeitig bei einer Datenmenge von 5000 Byte seine tatsächliche Verarbeitungsrate von 750 MIPS. Die grüne Kurve hingegen weist erst ab ca. 25.000 Byte einen ungefähr konstanten Wert auf. Der ECALL besteht insgesamt aus mehreren Funktionsaufrufen durch die Edge Routines, dessen Kern der eigentliche Funktionsaufruf in die Enclave bildet. Jener verhält sich wie ein Sprung in den Kernel Mode und ist mit entsprechendem Aufwand verbunden. So müssen etwa alte Registerinhalte gesichert und der eintretende logische Prozessor an eine Thread Control Structure gebunden werden. Somit nimmt die reine Prozedur des ECALLs deutlich mehr zeit in Anspruch und hat stets einen höheren Anteil an der Gesamtdauer als dies beim Funktionsaufruf in einer regulären Ausführung der Fall ist. Die Datenrate erreicht letztlich jedoch einen Wert von 680 MIPS und liegt somit nur 70 MIPS tiefer. Dies ist geringer als der Offset, welcher in der vorigen Untersuchung beobachtet wurde. Die Bedingungen der beiden Untersuchungen sind bezüglich des Kopiervorganges identisch, d.h. die Ausgabebuffer sind jeweils in beiden Fällen so groß wie die Eingabe, wodurch der Kopiervorgang die gleiche Zeit beansprucht. Ausschlaggebend sind jedoch die signifikant geringeren Datenraten des eigentlichen Verarbeitungsschrittes. Folglich nimmt der Kopiervorgang anteilig weniger Zeit am Gesamtprozess in Anspruch. Sobald auch die grüne Kurve den Grenzwert erreicht hat, macht jener quasi den einzigen Unterschied im Vergleich aus der beiden Durchgänge aus.

\paragraph{Einfache Kompressionsverfahren}

An dieser Stelle wird die Verarbeitung durch jeweils eines der in Kapitel 3 betrachteten Kompressionsverfahren, der einfachen Lauflängenkodierung und VByte übernommen. Getestet wurde zunächst jeweils die Dekompression und Kompression. Im Falle von VByte wurde zudem ein kombinierter Ablauf aus beiden Vorgängen untersucht. Auf diese Weise soll der Fall nachgebildet werden, dass die Basisdaten zum Zwecke des Einsparens an Speicherplatz zunächst komprimiert und nach der eigentlichen Verarbeitung wieder dekomprimiert werden, so wie dies im Schema in Abbildung \ref{fig:scenariocomp} betrachtet wurde.

Zum Testen der VByte Kompression wurden zufällig generierte Daten bereitgestellt, deren Größe ein Vielfaches von 4 Byte (32 Bit) aufweist. Dies ist nötig, da VByte in der eingesetzten Form auf entsprechend großen Integern arbeitet. Die Verläufe der Verarbeitungsgeschwindigkeit sind in \ref{fig:eval4} zu sehen. Da es sich bei den eingehenden Daten um zufällige Werte handelt, ist mit Sicherheit davon auszugehen, dass keine guten Kompressionsraten entstanden sind und die Berechnung stets entfernt davon war, im Best Case zu verlaufen. Die Anlaufphase der blauen Kurve ist erwartungsgemäß sehr kurz. Der Grenzwert wird sogar schon nach etwa 300 Byte erreicht. Verantwortlich ist auch hier die höhere Verarbeitungszeit, an welcher der Funktionsaufruf folglich weniger Anteile hat. Auf Seiten der sicheren Verarbeitung jedoch tritt dies erst bei 40.000 Byte ein. Von dort an beträgt der Offset jedoch nur noch geringe 30 MIPS, was sich wiederum auf die geringeren Verarbeitungsraten zurückführen lässt. Allerdings besitzt diese Untersuchung das Merkmal, dass der Ausgabebuffer nicht einfach nur gleich groß ist wie jener zur Eingabe. Da VByte im schlechtesten Fall einen 4 Byte Integer durch 5 Byte kodiert, muss für die Ausgabe ein Speicher reserviert werden, welcher 1.25 mal so groß ausfällt. Hierbei zeigt die durch SGX zur Verfügung gestellte Sicherheitsmaßnahme des Kopierens seine Schwäche, dass Ein- und Ausgabegrößen vor Aufruf des ECALLs bekannt sein müssen. Somit muss unabhängig von der resultierenden Größe komprimierter Daten der volle Buffer zurückkopiert werden.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval4.pdf}
	\centering
	\caption{Vergleich VByte Kompression}
	\label{fig:eval4}
\end{figure}

Das relativ geringe Verhältnis von Ausgabe- zu Eingabedaten sorgt im Zuge der Komprimierung noch nicht für schwerwiegende Auswirkungen auf das Offset der beiden Durchläufe. Bei einem Blick auf die Dekompression in \ref{fig:eval5} wird dieser Sachverhalt aber schon deutlicher. Im schlechtesten Fall wird hierbei ein einzelnes Byte zu einem ganzen 4 Byte Integer umgewandelt. Somit muss der Ausgabebuffer über die vierfache Größe relativ zur Eingabelänge verfügen. Der zusätzliche Kopieraufwand macht sich bemerkbar, da trotz einer hohen Verarbeitungsgeschwindigkeit der Dekompression, beträgt der Offset etwa 220 MIPS. Aufgrund der Eigenschaft des Algorithmus, dass die Datenrate sehr stark von der Form eingehender Daten abhängt, treten mitunter große Schwankungen in der Datenrate auf. Um dem etwas entgegenzuwirken wurden die Zufallsdaten sinnvollerweise vor Ausführung des Benchmarks vorverarbeitet, indem sie durch VByte komprimiert wurden. Dennoch lässt sich in der unsicheren Umgebung ein sehr unruhiger Verlauf erkennen. Im Falle der Enclave ist dies nicht der Fall. Hier besitzt das gleichmäßige Kopieren neben der reinen Verarbeitung einen großen Anteil an der Gesamtdauer des Vorgangs und sorgt für weniger Schwankungen.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval5.pdf}
	\centering
	\caption{Vergleich VByte Dekompression}
	\label{fig:eval5}
\end{figure}

Sowohl die Kompression als auch die Dekompression mittels VByte wurden nochmals in einem Ablauf zusammengefasst. In Abbildung \ref{fig:eval6} sind die entstandenen Verläufe zu sehen. Die eingehenden Daten werden zunächst komprimiert und daraufhin wieder dekomprimiert. Da in diesem vereinfachten Fall als Resultat stets die Eingangsdaten entstehen, ist der Ausgabebuffer ebenso groß wie jene. Im Diagramm kann man daher feststellen, dass die beiden Kurven einen sehr geringen Offset von etwa 20 MIPS aufweisen. Dies ist zusätzlich durch die vergleichsweise geringe Verarbeitungsrate von etwas mehr als 350 MIPS und der unsicheren Ausführung bedingt. Das Ergebnis zeigt schon einmal deutlich, dass die Berechnung in der Enclave eine durchaus vergleichbare Rate erzielen kann, sofern die durchgeführte Operation komplexerer Natur ist und der ausgehende Kopiervorgang möglichst klein ausfällt.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval6.pdf}
	\centering
	\caption{Vergleich VByte Kompression und Dekompression}
	\label{fig:eval6}
\end{figure}

Die Beobachtungen, welche unter dem Einsatz von VByte und der Iteration getroffen wurden, haben bereits viele Erkenntnisse über das Verhalten der sicheren Domäne in Bezug auf die Datenverarbeitung hervorgebracht. Im Gegensatz dazu weist die Kompression durch ein Lauflängenkodierungsverfahren unterschiedliche Merkmale auf. Der in diesem Kontext interessante Unterschied ist die hohe Variabilität der Kompressionsrate. Sowohl bei der Kompression, aber vor allem bei einer Dekompression ist es durchaus möglich, dass die resultierende Ausgabemenge eine sehr unterschiedliche Länge besitzt.

Ebenso wie bei der Kompression durch VByte besteht die Eingabe der Lauflängenkodierung aus 32 Bit Integerwerten. Der Ausgabebuffer muss doppelt so groß gewählt werden wie der Eingabebuffer. Das liegt daran, dass im ungünstigsten Fall jeder auftretende Wert verschieden seines Vorgängers ist und einen Lauflängenwert von 1 erhält. Jene Kennzeichnungen besitzen im eingesetzten Verfahren ebenso eine Größe von 4 Byte. In Abbildung \ref{fig:eval7} ist entsprechend zu erkennen, dass die Verläufe der Kurven um knapp 160 MIPS auseinanderliegen. Die Verarbeitungsraten in der regulären Anwendung ähneln jenen der VByte Dekompression, wodurch die Anlaufphasen der blauen und grünen Kurve ebenso bei jeweils ungefähr 5000 und 25.000 MIPS enden. Der Offset ist jedoch geringer, da das Verhältnis der Ausgabe- zur Eingabegröße nur halb so groß ausfällt. Bei etwa 3000 MIPS ist ein kleiner Einbruch der Datenrate zu erkennen, welcher durch einen anderen Prozess im Testsystem ausgelöst wurde. Jene Einbrüche sind in der Anlaufphase besonders auffällig, da die dafür verantwortlichen Unterbrechungen bei den anfänglich kleinen Gesamtzeiten einen größeren Einfluss besitzen.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval7.pdf}
	\centering
	\caption{Vergleich Lauflängenkodierung}
	\label{fig:eval7}
\end{figure}

Wie das Ergebnis in Abbildung \ref{fig:eval8} erahnen lässt, bringt die Lauflängendekodierung ein besonderes Problem mit sich. Der große Leistungsunterschied von der regulärer und sicherer Verarbeitung ergibt sich aus der Tatsache, dass der Ausgabebuffer sehr groß gewählt werden muss. Es lässt sich nur sehr schwer abschätzen, wie groß jener ausfallen muss, wenn die Form der zuvor komprimierten Daten nicht bekannt ist. Um keine unrealistisch hohen Lauflängen zuzulassen wurden die zufällig generierten Eingabedaten zusätzlich lauflängenkodiert. Dennoch wurde für die Ausgabe vorsichtigerweise jeweils ein Buffer gewählt, welcher 6-mal so groß ist wie die Eingabe. Als Schrittgröße wurden diesmal 8 Byte festgelegt, da die Dekodierung mindestens ein Paar von Lauflänge und Wert benötigt. Man kann anhand der erreichten Verarbeitungsrate von 2400 MIPS gut erkennen, dass das Verfahren grundsätzlich sehr schnell arbeitet. Im Unterschied zu den bisherigen Untersuchungen sorgt dies in Kombination mit dem aufwendigen Kopiervorgang dafür, dass die sichere Berechnung fast nur halb so schnell abläuft. Nach ca. 15.000 Byte erreicht sie die anhaltende Rate von 1100 MIPS. Man kann schlussfolgern, dass eine schnelle Verarbeitung durch einen langsamen Kopiervorgang signifikant ausgebremst wird.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{img/Eval8.pdf}
	\centering
	\caption{Vergleich Lauflängendekodierung}
	\label{fig:eval8}
\end{figure}

\section{Kryptographische Verarbeitung}
\begin{itemize}	
	\item \textbf{(10)} Entschlüsselung und Verschlüsselung außerhalb vs. in Enclave
	\begin{figure}[h]
		\includegraphics[width=\linewidth]{img/Eval9.pdf}
		\centering
		\caption{Vergleich Entschlüsselung und Verschlüsselung außerhalb und innerhalb der Enclave (ohne Kopieren)}
		\label{fig:eval9}
	\end{figure}
	
	\item \textbf{(11)} Entschlüsselung und Dekomprimierung außerhalb vs. in Enclave
	\begin{figure}[h]
		\includegraphics[width=\linewidth]{img/Eval10.pdf}
		\centering
		\caption{Vergleich Entschlüsselung und Dekomprimierung außerhalb und innerhalb der Enclave}
		\label{fig:eval10}
	\end{figure}
	
	\item \textbf{(12)} Komprimierung und Verschlüsselung außerhalb vs. in Enclave
	\begin{figure}[h]
		\includegraphics[width=\linewidth]{img/Eval11.pdf}
		\centering
		\caption{Vergleich Komprimierung und Verschlüsselung außerhalb und innerhalb der Enclave}
		\label{fig:eval11}
	\end{figure}
	
\end{itemize}

\section{Kombinierte Verarbeitungsschritte}

\begin{itemize}
	\item \textbf{(12)} Entschlüsseln/Dekomprimieren/Lauflängenkodierung/Komprimierung/Verschlüsseln
	\begin{figure}[h]
		\includegraphics[width=\linewidth]{img/Eval12.pdf}
		\centering
		\caption{Vergleich Verarbeitungsabfolge außerhalb und innerhalb der Enclave}
		\label{fig:eval13}
	\end{figure}

\end{itemize}

\section{Fazit}
%TODO notes

\begin{itemize}
	\item auf Messfehler eingehen
	\item Vermutung: Größe der Unterschiede zwischen den einzelnen Algorithmen voranging begründet durch den zu kopierenden Buffer am Ende (je Größer Verhältnis |Ausgabewerte|/|Eingabewerte|, je größer die Differenz)
\end{itemize}