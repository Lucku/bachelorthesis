
\chapter{Kompression}

Kompression ist ein weitverbreitetes Mittel zur Erzielung von Leistungsverbesserungen im Bereich der Datenverarbeitung. Einen zunehmenden Einsatz fanden sie vor allem in den immer stärker verbreiteten In-Memory Datenbanken, welche sämtliche Daten nicht wie zuvor üblich auf der Festplatte speichern, sondern den deutlich schneller verfügbaren Hauptspeicher nutzen.

Wie bereits im letzten Kapitel beschrieben, hat Intel mit seinen Secure Guard Extensions die Möglichkeit geschaffen, Daten innerhalb von sicheren Vertrauensbereiche (Enclaves) zu verarbeiten. Dies ermöglicht es mitunter, eine Umgebung zu schaffen, in welcher auf verschlüsselten Informationen ohne Beeinträchtigung der Vertraulichkeit im Klartext gerechnet werden kann. Der zusätzliche Einsatz von Kompressionsverfahren kann in diesem Szenario sinnvoll sein, um die Transferzeiten von Daten in eine Enclave, sowie den zeitlichen Verlust durch kryptographischen Operationen, je durch eine höhere Datendichte, zu verringern.

Im aktuellen Kapitel soll ein Einblick in den Einsatz und die Funktionsweise von Kompressionsverfahren in Datenbanksystemen gegeben werden. Zunächst wird etwas genauer auf die grundlegende Motivation ihres Einsatzes in diesem Kontext eingegangen. Verwandte Arbeiten zu dem Thema und eine damit verbundene zeitliche Darstellung der Forschungsentwicklungen wird nachfolgend beleuchtet, gefolgt von der Betrachtung konkreter Algorithmen. Letztere sind von besonderer Relevanz, da sie den Kern der Untersuchungen dieser Arbeit darstellen und später in einer SGX Enclave zum Einsatz kommen werden.

\section{Grundlagen} % möglicherweise Umbenennung in 'Motivation'

Durch das Verdichten von Daten, erlauben Kompressionsverfahren eine effizientere Nutzung der Ressource Speicher zugunsten der Datenverarbeitungszeit. Somit begünstigen sie eine bessere Ausnutzung der Speicherhierarchie, an deren Spitze die kleinen Prozessorregister mit schnellen Zugriffszeiten stehen und die Basis durch große, aber vergleichsweise sehr langsame Festplattenspeicher gebildet wird. Von Vorteil ist hierbei die Möglichkeit, eine höhere weil dichtere Datenmenge auf dem gleichen physischen Speicherplatz, etwa dem Prozessorcache, zu halten. Zudem werden Datentransferraten zwischen den einzelnen Stufen der Hierarchie erhöht \cite{Croft2009}. Zugrundeliegend ist ein \textit{Time-Memory Tradeoff}, denn die Einsparung an Speicherplatz kommt auf Kosten zusätzlicher Berechnungszeit dadurch, dass die Daten vor der eigentlichen Verarbeitung dekomprimiert werden müssen.

Durch ihre Arbeit auf dem Hauptspeicher nutzen In-Memory Datenbanken bereits eine höhere Stufe der Speicherhierarchie, als dies bei festplattenbasierten System der Fall ist. Um die Verarbeitungsgeschwindigkeit weiter zu verbessern, ist es nötig, die Transferzeiten von Daten zur CPU weitestgehend zu verringern. Eine größtmögliche Komprimierungsrate durch den Einsatz von Verfahren wie Huffman oder Lempel-Ziv anzustreben, erscheint aber keineswegs sinnvoll, da die zeitlichen Einsparungen in Bezug auf die zu transferierende Datenmenge durch den Zwang der aufwendigen Dekomprimierung zunichte gemacht werden \cite{Abadi2006}. Notwendig sind demnach leichtgewichtige Verfahren, sodass sich Daten schnell genug dekomprimieren lassen, um insgesamt echte Leistungsverbesserungen zu erzielen. Einige Verfahren wie die Lauflängenkodierung erlauben sogar die schnellere Verarbeitung auf komprimierten Daten selbst.

\section{Verwandte Arbeiten}

Die ersten Untersuchungen über den Einsatz von Kompressionstechniken in großen Datenbanken wurden 1982 veröffentlicht. Eine Gesamtverringerung der Datenmenge von 30-90\% war ein vielversprechendes Ergebnis, obwohl jene Verfahren zu jener Zeit noch kaum genutzt wurden \cite{Severance1982}. Des weiteren wurde gezeigt, dass gespeicherte Daten verschiedene Eigenschaften besitzen, welche manche Kompressionsverfahren eher begünstigen als andere.

In späteren Publikationen bedachte man nun nicht nur mehr die reine Datenhaltung, sondern auch die Nutzung zur effizienteren Datenverarbeitung im \textit{query processing}. Da eine Dekomprimierung vor Berechnungen obligatorisch ist, geht der Vorteil von Einsparung an Speicherplatz in gewissen Maßen verloren. Beschrieben wurden daher Kompressionsverfahren, welche es erlaubten, soweit wie möglich direkt auf den komprimieren Daten zu arbeiten um den verfügbaren Speicher möglichst effektiv zu nutzen \cite{Graefe1991}.

Allgemein adressierten frühere Ansätze vor allem traditionell relationale Datenbanksysteme, wobei die zu komprimierenden Einheiten etwa einzelne Tabellenreihen darstellten. Die in den letzten Jahren stark aufstrebenden spaltenorientierten Datenbanken bieten hingegen ein höheres Potenzial zur Kompression, dadurch, dass benachbart gespeicherte Werte in der Regel einer Spalte angehören und den gleichen Datentyp besitzen \cite{Abadi2006}. Besonders geeignet sind in diesem Fall Kompressionsschemata, welche auf Datenlokalität, also das enge Beieinanderliegen ähnlicher Daten, beruhen. Es wurde zudem gezeigt, dass das physische Datenbankdesign und das eingesetzte Verfahren zur Kompression auf einander abgestimmt sein müssen, um die besten Leistungsverbesserungen zu erzielen. Entsprechend spielt die Natur des zu erwartenden \textit{query workloads} eine entscheidende Rolle bei dessen Auswahl für jede individuelle Spalte.

\section{Algorithmen}

Die Grundlage von verlustfreier Kompression stellt die Darstellung einer Eingabesequenz an Daten in einer verkürzten Form dar, indem gewisse überschüssige Informationen, wie sie etwa durch Redundanz entstehen, reduziert werden. Es lassen sich verschiedene Herangehensweisen unterteilen, welche einzeln oder in kombinierter Form bei Kompressionsverfahren zum Einsatz kommen \cite{Abadi2006}\cite{Croft2009}:

\subparagraph{Nullunterdrückung}
Führende Leerstellen bzw. Nullen in der binären Repräsentation werden entfernt und durch entsprechende Angaben zu deren Anzahl und der Position des Auftretens ersetzt.

\subparagraph{Deltakodierung}
Gespeichert werden lediglich die Differenzen aufeinanderfolgender Werte, da diese in manchen Szenarien deutlich kleiner ausfallen, als die (absoluten) Werte selbst.

\subparagraph{Wörterbuchkodierung}
Es erfolgt eine eindeutige Abbildung häufig auftretender Sequenzen auf kürzere Codes.

\subparagraph{Lauflängenkodierung}
Abfolgen gleicher Werte werden durch eine einzige Darstellung bestehend aus Wert und Lauflänge ersetzt.

\subparagraph{Bitvektorkodierung}
Anstatt der Folge von Werten selbst, werden jeweils die Positionen des Auftretens jedes Wertes kodiert, sodass jeder Wert einen individuellen Bitvektor besitzt.

\paragraph{}
In diesem Unterkapitel soll mit VByte ein konkreter einfacher Algorithmus behandelt werden, welcher sich die Nullunterdrückung zunutze macht. Anschließend wird ein möglicher Ansatz zur Lauflängenkodierung untersucht und auf seine besonderen Eigenschaften bei der Datenverarbeitung eingegangen.

\subsection{VByte}

Bei VByte (Abkürzung für \textit{variable byte length}) handelt es sich um ein äußerst schnelles Verfahren, welches kleinere Zahlenwerte kürzer kodiert als längere. Der Einsatz eignet sich demzufolge immer dann, wenn die vorliegende Eingangssequenz aus vielen kleineren Integern besteht. Der Algorithmus arbeitet byteorientiert, wodurch die Ausgabe stets ein Vielfaches eines Bytes ist und der kleinstmögliche Code genau ein Byte lang ist. Als Eingabe dienen 32 Bit Integerwerte, welche auf bis zu 5 Byte abgebildet werden.

Das generelle Vorgehen bei der Komprimierung besteht aus dem Prüfen der numerischen Größe des 32 Bit Wertes. Hierdurch kann eine Aussage über die Anzahl führender Nullen getroffen werden, welche es zu entfernen gilt. Um zu Zwecken der späteren Dekomprimierung zu kennzeichnen, welche Bytes durch die Nullunterdrückung entfallen sind, wird jedes Ausgabebyte durch 7 Datenbits und einen Deskriptor im obersten Bit kodiert. Letzteres dient der Anzeige des aktuellen Laufs: eine 1 bedeutet, dass das Byte noch Teil des aktuellen Wertes ist, während eine 0 den Anfang eines neuen Integers indiziert. Entsprechend werden jegliche Werte kleiner gleich $2^7$ durch ein Byte kodiert, jene kleiner gleich $2^{14}$ durch zwei Byte und so weiter. In Abbildung \ref{fig:vbyte} wird die beispielhafte Komprimierung des Integers 0x000D9E5B (32 Bit) in der hexadezimalen Repräsentation gezeigt. Durch Einsparung des ersten Bytes und Berücksichtigung der zusätzlichen Deskriptorbits (grau gekennzeichnet) resultiert die Ausgabe 0x36BCDB (24 Bit).

\begin{figure}
	\includegraphics[width=\linewidth]{img/VByte.pdf}
	\centering
	\caption{Beispiel zur VByte Kompression}
	\label{fig:vbyte}
\end{figure}

\subsection{Lauflängenkodierung}

Anders als VByte sind Verfahren der Lauflängenkodierung (oder RLE - \textit{Run Length Encoding}) nicht zwangsläufig byteorientiert. Der Einsatz des Verfahrens ist immer dann sinnvoll, wenn Werte oftmals wiederholt nacheinander auftreten, etwa in einer der Größe nach sortierten Folge von Integern. Entsprechende Redundanz kann genutzt werden, um jene Sequenzen in Läufe zu unterteilen. Anstatt die aufeinanderfolgenden wiederholten Werte speichert man nur noch Tupel der Form (Wert, Lauflänge). In Abbildung \ref{fig:rle} wird eine beispielhafte Komprimierung der Folge 11123344555 durchgeführt. Man kann leicht erkennen, dass das einfache Auftreten eines Wertes in einem jeweils längeren Code resultiert und eine echte Verkürzung erst ab der Lauflänge 3 erfolgt. Anders als in dem abgebildetem Schema, müssen die Lauflängenwerte jedoch nicht die gleiche Größe wie die Ausgangswerte haben. Je nachdem wie wahrscheinlich die maximal zu erwartende Lauflänge ist, können sie deutlich kleiner gewählt werden, um die Kompressionsrate weiter zu erhöhen. Es existieren auch weitere Optimierungsmöglichkeiten wie beispielsweise das Einsparen der Komprimierung bei kleineren Lauflängen.

\begin{figure}
	\includegraphics[width=0.9\linewidth]{img/RLE.pdf}
	\centering
	\caption{Beispiel zur RLE Kompression}
	\label{fig:rle}
\end{figure}

Sofern eine Integerwertefolge durch die Komprimierung mittels RLE verdichtet wurde, wird in gleichem Maße die Bildung des Summenaggregats begünstigt. Anstatt die Werte einzeln aufzusummieren, bietet sich hierbei ein direktes Arbeiten auf den komprimierten Werten an, da Additionen durch die Berechnung von $Wert * Laufl\ddot{a}nge$ eingespart werden können. Im Beispiel von Abbildung \ref{fig:rle} ergeben sich 10 Operationen durch die Berechnung auf den unkomprimierten Werten:

\begin{equation*}
	1 + 1 + 1 + 2 + 3 + 3 + 4 + 4 + 5 + 5 + 5 = 34
\end{equation*}

Für entsprechende Berechnung auf komprimierten Werten sind nur 9 Operationen notwendig, während Berechnungskosten für die Dekomprimierung entfallen:

\begin{equation*}
	1 * 3 + 2 * 1 + 3 * 2 + 4 * 2 + 5 * 3 = 34
\end{equation*}

\paragraph{}
In diesem und dem vorhergehenden Kapitel wurden mit der Sicherheit in Datenbanksystemen und der Kompression als Hilfsmittel in der Datenverarbeitung die grundlegenden Konzepte dieser Arbeit untersucht. Neben verschiedenen Ansätzen zur Gewährleistung der Vertraulichkeit wurde Intel SGX als aktuelle auf Hardware basierende Entwicklung eingeführt, welche sich die Definition von Enclaves als sichere, nur durch den Prozessor zugreifbare Vertrauensbereiche zunutze macht.
Die Platzbeschränkungen sowie Transferzeiten von Daten in jene Bereiche gestalten sich als Herausforderungen, welchen durch eine Datenkompression entgegengewirkt werden kann. Daher wurde eine entsprechende Einführung in den bestehenden Einsatz von Kompressionsverfahren innerhalb datenverarbeitender Systeme gegeben und einige Herangehensweisen zu deren Umsetzung erklärt. Zur Nutzung in den folgenden Untersuchungen dieser Arbeit waren abschließend zwei konkrete Algorithmen, VByte und ein Lauflängenkodierungsverfahren, im Fokus.
%TODO Outro ggf. überarbeiten, um die Arbeit richtig abzurunden