
\chapter{Einleitung}

Die Erhebung und Speicherung von Daten ist eine seit den letzten Jahren ständig wachsende Entwicklung. "`Big Data"' ist eines der zentralen Schlagworte des Bereichs Data Science und umschreibt jenen Prozess. Um den großen Mengen an Daten Herr zu werden, sind immer umfangreichere Speicher- und Rechenkapazitäten nötig. Viele Unternehmen setzen daher schon seit einigen Jahren auf die Nutzung von Diensten in der Cloud, um jene erforderliche Infrastruktur auszulagern. Die Wartung von Servern wird dabei einem unabhängigen Drittanbieter überlassen. Dieses als Database-as-a-Service betitelte Modell bringt für die Firmen neben den Vorteilen der Entlastung aber auch große Risiken mit sich, denn in vielen Fällen handelt es sich um die Speicherung und Verarbeitung von sensitiven Daten, welche sich fortan auf nicht vertraulichen Systemen befinden. Ein offensichtliches Vorgehen ist die komplette Verschlüsselung der Datenbanken. Dies stellt sich aber als äußerst unpraktikabel heraus, da jene Daten auch verarbeitet werden müssen, also lesend und schreibend auf sie zugegriffen werden muss. Dies sollte jedoch aufgrund der potenziellen Komplexität nicht erst auf dem anfragenden Client geschehen.

In der Vergangenheit haben sich zwei generelle Ansätze zur Lösung dieses Problems herausgestellt. Zum einen setzt man auf spezielle Verschlüsselungsverfahren, welche Berechnungen auf Schlüsseltexten erlauben, jedoch auf Kosten der kryptographischen Sicherheit, des Berechnungsaufwands oder der Funktionalität. Auf der anderen Seite versucht man, Daten in gewisse Vertrauensbereiche zu transferieren, um sie dort sicher verarbeiten zu können. Oftmals handelt es sich hierbei um sichere Hardwarekomponenten. Innerhalb jener Bereiche entschlüsselt man die Daten, führt entsprechende Berechnungen auf ihnen aus und verschlüsselt sie wieder, bevor sie den Bereich verlassen. Die Nachteile hierbei liegen im oftmals stark eingeschränkten Speicherplatz, der für Berechnungen zur Verfügung steht. Eine Kombination der zwei Herangehensweisen vermindert die Nachteile auf beiden Seiten, und wird von den meisten aktuellen Ansätzen genutzt.
 
Intel stellt mit den Software Guard Extensions (SGX) eine Menge von Erweiterungen für ihre Prozessorarchitektur bereit, welche das Ziel verfolgen, die sichere Datenverarbeitung zu ermöglichen. Sie erlauben es dem Entwickler sogenannte Enclaves zu definieren. Dies sind Vertrauensbereiche, in welchen eine sichere Berechnung auf Daten ermöglicht wird, während sämtliche privilegierte Software im System, etwa der Betriebssystemkern, gleichzeitig korrumpiert sein kann. Lediglich der Prozessor hat die Möglichkeit, Einsicht in die Daten innerhalb von Enclaves zu erhalten. Somit lässt sich Intel SGX klar in die Kategorie der auf Vertrauensbereichen basierenden Ansätze einordnen.

Während herkömmliche Einsatzfälle sich eher darauf beschränken, einzelne sensible Daten wie Passwörter in einer Enclave zu halten, soll Intels Technologie in dieser Arbeit im Kontext einer In-Memory Datenbank eingesetzt werden. Es handelt sich hierbei um Systeme, welche hauptsächlich innerhalb des Hauptspeichers arbeiten und somit eine deutlich schnellere Datenverarbeitung erlauben. Das Ziel ist es, auf Grundlage konkreter Fragestellungen zu untersuchen, ob es  möglich ist die komplette Anfrageverarbeitung einer solchen Datenbank in eine sichere Enclave zu verlagern und ob es aus Sicht des zusätzlichen Bearbeitungsaufwandes Sinn ergibt. Interessant ist etwa, ob der zeitliche Overhead im Gegensatz zu einer herkömmlichen, aber unsicheren Verarbeitung vernachlässigbar ist. Um die Berechnungsgeschwindigkeit innerhalb einer Enclave zu maximieren, ist es unter Umständen nötig, sehr große Datenmengen gleichzeitig zu halten. Wie bereits in In-Memory Datenbanken gängig, soll ein auf diese Weise potenziell auftretendes Platzproblem durch den Einsatz von leichtgewichtigen Kompressionsverfahren gelöst werden. Der Aufwand von Datenkompression könnte um einiges geringer sein, als ein ständiges Transferieren neuer Daten in die Enclave. Zudem existieren Verfahren wie die Lauflängenkodierung, welche ein teilweise schnelleres Rechnen auf komprimierten Daten erlauben. Jene Kompressionsverfahren kommen während der Untersuchungen zum Einsatz.

Für die Bearbeitung dieses Themas ist die Arbeit grundsätzlich zweigeteilt. Der erste Teil umfasst Kapitel 2 und 3, welche zunächst eine theoretische Grundlage legen sollen. In Kapitel 2 wird zum ersten auf das Thema der Sicherheit in Datenbanken eingegangen. Im Vordergrund steht die Betrachtung zentraler Begriffe aus der Datensicherheit und ein konzeptioneller Überblick über Intel SGX. Zudem werden verwandte Arbeiten behandelt, die sich ebenso eine sichere Datenverarbeitung zum Ziel gesetzt haben. Das folgende Kapitel handelt vom Begriff der Datenkompression, und setzt diesen in den Kontext der Arbeit. Auch hier werden grundlegende Konzepte erklärt und auf verwandte Arbeiten eingegangen. Mit VByte und der Lauflängenkodierung werden zudem konkrete Kompressionsalgorithmen behandelt, welche für spätere Untersuchungen relevant sind. Der zweite große Teil umfasst Kapitel 4-6 und ist primär praktisch orientiert. Im Fokus stehen hier die eigenen Untersuchungen, angefangen mit Kapitel 4 und einer Beschreibung der Implementierung einer Testumgebung unter Nutzung von SGX. Die Auswertung der entstandenen Ergebnisse erfolgt nachfolgend in Kapitel 5. Abschließend werden in einem Fazit die zentralen Erkenntnisse zusammengefasst und ein Ausblick auf weiterführende Themen und Fragestellungen gegeben.