
\chapter{Sicherheit in Datenbanken}

Um grundsätzlich ein System in Bezug auf seine (Informations-)sicherheit zu betrachten, ist es notwendig, dieses auf die Einhaltung gewisser Merkmale hin zu charakterisieren. Diese als Schutzziele bezeichneten Eigenschaften fallen je nach Kontext anders aus. Im Allgemeinen bestehen sie jedoch aus den drei Begriffen Vertraulichkeit, Integrität und Verfügbarkeit:

\paragraph{Vertraulichkeit}
Als Vertraulichkeit bezeichnet man die Eigenschaft, dass Informationen in einem System nur denjenigen Subjekten zur Verfügung steht, welche dazu berechtigt sind.

\paragraph{Integrität}
Integrität liegt vor, wenn jegliche Modifikation einer Datenmenge zu jeder Zeit erkannt werden kann.

\paragraph{Verfügbarkeit}
Ein System besitzt Verfügbarkeit, wenn es fortlaufend unter korrekter Funktionsweise arbeitet.

\paragraph{}
Dieses Kapitel beschäftigt sich zunächst mit dem konventionellen Vorgehen zur Gewährleistung der Schutzziele in Datenbanksystemen. Um die Problematik dieser Arbeit aufzugreifen, wird daraufhin auf die Sicherheitsaspekte von Datenbanklösungen in nicht vertrauenswürdigen Umgebungen eingegangen, wobei relevante Begriffe erklärt werden. Nachfolgend erfolgt eine Darstellung entsprechender Lösungskonzepte aus anderen Publikationen. Zum Abschluss des Kapitels wird der Fokus auf das Konzept und den grundlegenden Aufbau von Intel SGX, den in dieser Arbeit untersuchten Ansatz, gelegt.

\section{Konventionelles Lösungskonzept}

Um Sicherheit in einem Datenbanksystem zu erreichen, setzte man klassischerweise auf ein schichtweise organisiertes Konzept, dessen Basis durch den physischen Schutz der Systemhardware gebildet wird. Die nächsthöhere Ebene bildet das Betriebssystem, welches wiederum eigene Maßnahmen zur Verfügung stellt, um das Datenbanksystem zu schützen. Letzteres bildet die oberste Ebene und gleichzeitig die Schnittstelle zum Benutzer. Jede Ebene ist auf die Gewährleistung von Sicherheit aus der jeweils unterliegenden Schicht angewiesen. Gelingt einem Angreifer etwa der physische Zugriff auf das System, so sind weder die Maßnahmen des Betriebssystems, noch des Datenbanksystems wirkungsvoll. Entsprechender Schutz wird durch klassische Mittel wie dem Einsatz von Sicherheitspersonal, Alarmsystemen und sonstiger physischer Zugriffskontrolle der Hardware gewährleistet. Auf der Ebene des Betriebssystems werden Authentisierungsverfahren genutzt, um den Zugriff abzusichern. Üblicherweise handelt es sich hierbei um Anmeldedaten, welche aus einer Nutzeridentifikation und Passwort bestehen. Außerdem stehen \textit{Auditing Services} zur Verfügung, welche es erlauben, Änderungen des Systemzustandes anhand von Logs nachzuvollziehen, was der Einhaltung von Integrität zugute kommt. Zuletzt ist auch das Datenbanksystem selbst dafür zuständig, die eigene Sicherheit zu schützen. Die Funktionalitäten des Betriebssystems sind hierfür allein nicht ausreichend, was vor allem dadurch begründet ist, dass oftmals komplexe Beziehungen zwischen den gespeicherten Daten zusätzliche Semantik hinzufügen, welche in der unterliegenden Ebene nicht existieren. Die Organisation der Datenobjekte erfolgt entsprechend auf einer höheren Abstraktionsebene, als dies innerhalb des Betriebssystems in Form von Dateien der Fall ist \cite{Vimercati2001}. Zur Gewährleistung von Vertraulichkeit und Integrität sind demnach auch hier eine gesonderte Zugriffskontrolle und Auditing notwendig.

Die einfache Zugriffsregelung zum Schutz der Vertraulichkeit stellt eine große Schwäche des betrachteten Konzeptes dar, da ein privilegierter Nutzer, etwa ein böswilliger Datenbank- oder Systemadministrator, die Einsicht über jegliche Daten besitzt. Vor allem aber der verstärkt genutzte Einsatz von Datenbanklösungen in einer nicht vertraulichen Cloudumgebung machen jenen Ansatz obsolet, da sich das genutzte \ac{DBMS} auf dem System des jeweiligen Anbieters befindet. Die untergeordneten Schichten sind demzufolge außerhalb der eigenen Kontrolle, woraus sich die Notwendigkeit ergibt, dass sich allein das Datenbanksystem um die Einhaltung von Vertraulichkeit und Integrität kümmern muss. In Abbildung \ref{fig:dbsecurity} ist die Relation zwischen den drei Schichten \ac{DBMS}, Betriebssystem und physischer Hardware dargestellt, wobei das klassische mit einem cloudbasierten Szenario (Database-as-Service) verglichen wird. Umrahmte Bereiche stellen den jeweiligen Einflussbereich dar. Rot markierte Stufen der Hierarchie sind wiederum potenziell unter der Kontrolle eines Angreifers.

\begin{figure}
	\includegraphics[width=0.8\linewidth]{img/DBSecurity.pdf}
	\centering
	\caption{Einflussbereiche in der Systemhierarchie}
	\label{fig:dbsecurity}
\end{figure}

\section{Datenbanksicherheit in nicht vertrauenswürdigen Umgebungen}

Im Kontext dieser Arbeit wird konkret ein Datenbanksystem betrachtet, welches sich zwar unter der Kontrolle einer berechtigten Partei, jedoch in einem nicht vertrauenswürdigen Umfeld befindet. Da es hierbei hauptsächlich um die Vertraulichkeit in Bezug auf sensitive Daten geht, werden Integrität und Verfügbarkeit im Folgenden stets außer Acht gelassen. Die Speicherung jener Daten auf einem Datenbankserver offenbart allgemein drei kritische Szenarien, welche zum Schutz der Vertraulichkeit gezielt betrachtet werden müssen \cite{Shmueli2010}. Hierbei wird zunächst angenommen, es handelt sich um ein klassisches Datenbanksystem, welches Daten vordergründig auf der Festplatte speichert.

\paragraph{Data-in-motion}
Die Kommunikation zwischen Client und Server, wobei standardmäßige Kommunikationsprotokolle zum Einsatz kommen.

\paragraph{Data-in-use}
Daten, welche zum aktuellen Zeitpunkt verarbeitet werden, liegen im Hauptspeicher und können dort durch einen Angreifer ausgelesen werden.

\paragraph{Data-at-rest}
Gespeicherte Daten befinden sich auf der Festplatte und müssen vor unbefugtem Zugriff geschützt werden.

\paragraph{}
Aufgrund standardisierter Protokolle zur sicheren Client-Server Kommunikation (\acs{SSL}/\acs{TLS}) ist der erste Punkt heutzutage weitestgehend unkritisch und in unserem Kontext uninteressant. Um den Vertraulichkeitsschutz von Daten im Hauptspeicher zu gewährleisten, haben sich in der Vergangenheit zwei allgemeine Vorgehensweise etabliert. Zum einen ist dies die Einbeziehung von sicheren Hardwarekomponenten in den Berechnungsprozess. Diese agieren als sogenannte Vertrauensbereiche und erlauben ein Arbeiten auf den Klartextdaten, ohne dass ein potenzieller Angreifer einen Einblick in diese bekommen kann. Zusammen mit der Software, die innerhalb von ihnen ausgeführt wird, bilden diese Bereiche die sogenannte \ac{TCB} des Systems und sollten frei von Fehlern sein, welche mögliche Sicherheitslücken eröffnen können. Auf der anderen Seite wurden eigenschaftserhaltende Verschlüsselungsverfahren entwickelt, welche im Gegensatz zu üblichen, randomisierten Methoden eine gewisse Teilmenge an Berechnungen auf Schlüsseltexten erlauben. Folglich kann in einzelnen Verarbeitungsschritten darauf verzichtet werden, auf die Klartextrepräsentation zugreifen zu müssen, während gewisse Sicherheitsgarantien verloren gehen. Zum Schutz von Data-in-Rest kommen ebenfalls verschiedene Ausprägungen von Verschlüsselung zum Einsatz. Bei Betrachtung von In-Memory Datenbanken existiert eine leichte Abweichung zur Definition der obigen Szenarien. In diesem Fall befinden sich neben den Zwischenergebnissen der Datenverarbeitung auch die Basisdaten, also Data-at-rest, im Hauptspeicher.

Das Integrieren von Sicherheitsmechanismen in Form von Verschlüsselung geht in der Regel mit einem signifikanten rechnerischen Overhead einher. Da dieser bis auf ein Minimum reduziert werden sollte, ergeben sich gewisse Anforderungen an den Einsatz jener Verfahren \cite{Shmueli2010}. Zunächst ist es sinnvoll, nur jene Daten bzw. Informationen zu verschlüsseln, welche als sensitiv gelten. Unsensible Daten können weiterhin in Klartextform vorliegen, da ihre unfreiwillige Offenbarung keinen Schaden verursacht. Des Weiteren sollen während  der Datenverarbeitung nur die Daten ver- bzw. entschlüsselt werden, welche zum Zwecke der aktuellen Anfrageausführung relevant sind. Ein dritter Punkt ergibt sich mit dem Einsatz des passenden Verschlüsselungsschemas für die gewünschten Anforderungen. Konkret gilt es, einen Kompromiss zwischen der durch das Verfahren gewährleisteten Sicherheit und der trotzdem zur Verfügung stehenden Funktionalität und Leistung zu finden, da sich beides in der Regel entgegensteht. Zuletzt muss darauf geachtet werden, dass der entstehende Speicherplatzoverhead von möglichst geringem Ausmaß sein sollte.

\section{Bestehende Lösungsansätze}

Auf dem Gebiet der sicheren Datenhaltung und -verarbeitung auf Datenbankebene gab es in der Vergangenheit bereits vielversprechende Lösungsansätze. In diesem Kapitel wird ein Überblick über entsprechende Arbeiten gegeben. Die bedeutendsten Vertreter werden zudem etwas genauer auf ihre Vor- und Nachteile untersucht. Hierbei sind die Hauptmerkmale vor allem die zur Verfügung gestellte Sicherheit unter dem kryptographischen Gesichtspunkt und auch der Möglichkeit zur Anpassbarkeit. Darüber hinaus stellt sich auch die Frage, inwieweit die Funktionalität des Datenbanksystems beeinflusst wird, d.h. ob es eine Einschränkung der Datenbankschnittstelle gibt. Ein Beispiel wäre der Umfang zur Verfügung stehender Anfragen. Weiterhin ist interessant, ob starke Kompromisse auf leistungstechnischer Ebene eingegangen werden müssen.

Ein grundlegender Ansatz, den alle betrachteten Arbeiten gemeinsam haben, ist eine Verschlüsselung der zugrundeliegenden Datenbank. Eine Differenzierung der Lösungsansätze erfolgt daher im Folgenden nach den im letzten Unterkapitel aufgeführten Ansätzen zum Schutz von Data-in-use. Zunächst wird auf Arbeiten eingegangen, welche vorrangig auf die Einbeziehung von sicherer Hardware beruhen. Betrachtet werden hierbei vor allem die bekanntesten Vertreter TrustedDB und Cipherbase. Nachfolgend wird mit CryptDB ein rein auf eigenschaftserhaltender Verschlüsselung basierender Ansatz behandelt, welcher den Grundstein für die weiteren aufgeführten Arbeiten MONOMI, Talos und Arx bildet. In Abbildung \ref{fig:timeline} ist eine einfache zeitliche Einordnung der verschiedenen Arbeiten zu sehen.

\begin{figure}
	\includegraphics[width=0.9\linewidth]{img/RelatedWorkTimeline.pdf}
	\centering
	\caption{Zeitliche Einordnung verwandter Arbeiten nach Kategorie}
	\label{fig:timeline}
\end{figure}

TrustedDB \cite{Bajaj2013} erweitert den üblichen Datenbankserver um die zusätzliche Einbeziehung eines sicheren Koprozessors (SCPU), wie diese zum Beispiel von der Firma IBM hergestellt wird, um eine gesicherte Datenverarbeitung zu gewährleisten. Bei der Definition des Datenbankschemas können einzelne zu schützende Attribute entsprechend ausgezeichnet werden. Anhand von \ref{fig:trusteddb} wird die Funktionsweise des Verfahrens vereinfacht erklärt. 

\begin{figure}[h]
	\includegraphics[width=0.9\linewidth]{img/RelatedWorkTrustedDB.pdf}
	\centering
	\caption{Ablauf einer Datenbankanfrage unter TrustedDB}
	\label{fig:trusteddb}
\end{figure}

Bei einer Anfrage des Clients wird diese zunächst verschlüsselt, bevor sie zum Server gelangt (1). Der unsichere Teil des Servers kann die Anfrage in der Form nicht bearbeiten und leitet sie an die SCPU weiter. Nach der Entschlüsselung findet dort eine Unterteilung in Teilanfragen statt, unter dem Gesichtspunkt, ob diese die Verarbeitung sensibler oder harmloser Daten beinhalten (2). Resultat ist eine Menge sogenannter \textit{Public} und \textit{Private Queries}. Erstere kann die herkömmliche Datenbankengine übernehmen (3). Dies ist in der Regel ein deutlich größerer Anteil der Daten. Die SCPU übernimmt nur die private queries, also nur den Teil der Arbeit, für den sie tatsächlich bestimmt ist (4). Die Ergebnisse werden zum Schluss zusammengefügt und verschlüsselt zum Client zurückgeschickt (5), der diese wiederum entschlüsselt (6). Als großer Vorteil wird unter anderem die bessere Leistung gegenüber kryptographischer Berechnungen in Software beschrieben. Zudem gibt es keine Einschränkungen an möglichen \ac{SQL} Anfragen. Jedoch erfordert TrustedDBs Architektur das Einbetten eines gesamten Datenbanksystems in die SCPU und somit in die Trusted Computing Base. Aufgrund der beschränkten Ressourcen des Koprozessors muss hier zudem auf eine in Funktionalität ärmere SQLite Datenbank zurückgegriffen werden \cite{Arasu}. Sobald ein größerer Anteil der Daten als sensitiv eingestuft und die SCPU somit in größerem Maße beansprucht wird, nimmt die Leistung stark ab, da dessen Rechenleistung eingeschränkt ist \cite{Arasu2012}.

Im Gegensatz zu TrustedDB setzt Cipherbase \cite{Arasu2012}\cite{Arasu} auf einen sehr eng gekoppelten Ansatz und verspricht somit ein deutlich leistungsfähigeres System. Es erweitert ursprünglich den Microsoft \ac{SQL} Server um eine \textit{Trusted Machine}. Eine Besonderheit ist die Umsetzung jener als einfache Stapelmaschine, auf welcher ausschließlich die Berechnung der kryptographischen Primitive abläuft. Zur Realisierung der Trusted Machine eignen sich zum Beispiel \acsp{FPGA} \cite{Arasu}. Kombiniert wird das ganze mit dem Einsatz einer eigenschaftserhaltenden Verschlüsselung, wie diese in später aufgeführten Verfahren wie CryptDB betrachtet wird. Man setzt hierbei auf ein statisches Modell, wodurch Data-at-rest einmalig mit der benötigten Methode unter Gewährleistung bestimmter Funktionalität, z.B. der Möglichkeit für Bereichsanfragen, verschlüsselt werden. Das enge Hardware Software Codesign erlaubt eine bessere Leistung gegenüber TrustedDB, während die \ac{TCB} möglichst klein gehalten wird \cite{Arasu}. Zudem gibt es keine Limitierungen in Bezug auf die unterstützte Ausdrucksmächtigkeit von Anfragen, da ein einziges umfangreiches Datenbanksystem erweitert wird \cite{Arasu2013}. Als Nachteil ist hier lediglich die Minderung gewisser Sicherheitsmerkmale im Zuge der eigenschaftserhaltenden Verschlüsselung zu nennen.

Auf Seiten der Verfahren, welche rein auf den eingesetzten Verschlüsselungsverfahren beruhen, ist an erster Stelle CryptDB \cite{Popa2011}\cite{Popa2012} zu nennen. Genau wie die zuvor behandelten Systeme erfolgte ein Entwurf für die Arbeit auf relationalen \ac{SQL} Datenbanken. Außerdem wurde es ebenso wie TrustedDB 2011 publiziert, wodurch die beiden Vorgehensweisen stets koexistierten. Der Kern von CryptDBs Architektur ist ein Proxy Server, der sich logisch gesehen zwischen Anwendungs- und Datenbankserver befindet, im Normalfall aber ebenfalls auf dem Anwendungsserver läuft. Sobald eine Anfrage an die verschlüsselte Datenbank gestellt wird oder eine Antwort zurückkommt, verlaufen diese zunächst über jenen Proxy. Ein vereinfachtes Schema der Architektur ist in Abbildung \ref{fig:cryptdb} zu sehen. CryptDB nutzt den Aufbau von \ac{SQL} Anfragen aus mehreren einfachen Operationen wie Joins, Gleichheitsprüfungen oder Ordnungen auf Werten aus, um diese direkt auf den verschlüsselten Daten durchführen zu können. Das entsprechend eingesetzte Schema wird als \textit{SQL aware encryption} bezeichnet. Es definiert kryptographische Methoden, unter deren Einsatz der Erhalt einer bestimmten, auszuführenden Teilmenge von Operationen gewährleistet wird. Ist es beispielsweise nur nötig, eine Selektion mit Gleichheitsbedingung durchzuführen, so genügt ein deterministisches Verschlüsselungsverfahren, welches stets den gleichen Schlüsseltext für einen gegebenen Klartext produziert. Will man allerdings den minimalen oder maximalen Wert bzw. eine geordnete Ergebnismenge erhalten, so muss ein schwächeres Verfahren eingesetzt werden, welches die Ordnung von Klartextwerten durch das Verschlüsseln erhält, sogenannte \ac{OPE}. Um eine möglichst große \ac{SQL} Funktionalität neben einem höchstmöglichen Maß an Vertraulichkeit bereitzustellen, setzt CryptDB auf ein sogenanntes \textit{Onion Scheme}. Data-at-rest ist in mehreren Schichten verschlüsselt, wobei die gegebene Sicherheit mit jeder Schicht zunimmt. Ebenso nehmen die möglichen Operationen ab. Entschlüsselt wird dann nur bis zu der Schicht, welche für die aktuellen Berechnungen erforderlich ist. Die benutzten Schlüssel liegen je Nutzer auf dem Anwendungsserver. Der Proxy selbst besitzt jedoch einen Masterkey, um Anfragen umschreiben zu können. Da jeder Benutzer des Systems über einen individuellen Schlüssel verfügt, ist es möglich, gewisse Daten für einzelne Nutzer freizugeben oder zu verstecken, wofür der Proxy ein spezielles Schema hält. Die Datenverarbeitung selbst erfolgt durch \acp{UDF} auf dem Datenbankserver, nachdem die eingehenden Anfragen umgeschrieben wurden. Infolge der Auslieferung von den verschlüsselten Ergebnisdaten an den Proxy kann dieser die Entschlüsselung vornehmen und sie dem Anwendungsserver liefern.

\begin{figure}
	\includegraphics[width=0.9\linewidth]{img/RelatedWorkCryptDB.pdf}
	\centering
	\caption{Architektur von CryptDB}
	\label{fig:cryptdb}
\end{figure}

Trotz der zusätzlichen Bearbeitung im Proxy hat CryptDB einen relativ geringen Overhead an Rechenaufwand im Vergleich zum herkömmlichen unsicheren Ablauf \cite{Popa2012}. Die Nachteile des Verfahrens liegen allerdings in den eingeschränkten Möglichkeiten an \ac{SQL} Anfragen sowie den fehlenden Konfigurationsmöglichkeiten in Bezug auf die gewünschte Sicherheit. Eine absichtliche Nutzung schwächerer Verschlüsselungsverfahren wie \ac{OPE} erleichtert einem Angreifer, Aussagen über die Schlüsseltexte zu treffen \cite{Poddar2016}. Als großer Nachteil ist auch zu nennen, dass im Gegensatz zu Cipherbase keine freie Konfiguration der Sicherheit je Spalte möglich ist.

Zwei Jahre nach der Veröffentlichung von CryptDB folgte mit MONOMI \cite{Tu2013} ein System, welches auf dem gleichen Verschlüsselungsschema aufbaute, jedoch für den Anwendungsfall von analytischen Workloads angepasst wurde. Diese unterscheiden sich größtenteils dadurch, dass viel größere Datenmengen auf komplexe Art verarbeitet werden. In einer komplett verschlüsselten Datenbank bringt dies viele Probleme mit sich, unter anderem, wie jene komplexen Anfragen effizient auf Schlüsseltexten ausgeführt werden können. Die Lösung von MONOMI besteht aus einer Verteilung der Anfrageausführung auf Server und Client. Bei Letzterem können somit letzte Verarbeitungsschritte ausgeführt werden, nachdem die Daten schon unverschlüsselt sind, während jene auf dem Server mit den verschlüsselten Daten deutlich aufwendiger wären. MONOMI bringt zudem noch weitere Leistungsoptimierungen mit und schneidet bei großen Benchmarks sowohl in Bezug auf Speicherbedarf und Leistung deutlich besser ab als CryptDB \cite{Tu2013}.

An dieser Stelle muss auch Talos \cite{Shafagh2015} erwähnt werden. Das 2015 beschriebene System vereinfacht und optimiert das Konzept von CryptDB für den Einsatz im Internet of Things. Die eingesetzten kryptographischen Algorithmen z.B. für \ac{OPE} werden durch leistungsfähigere Varianten ersetzt und erlauben die Ausführung auf Systemen mit beschränktem Energiebudget und Speicherplatz.

Der letzte hier genannte Vertreter in der Reihe verwandter Arbeiten ist Arx \cite{Poddar2016}. Es handelt sich um einen ganz offiziellen Nachfolger von CryptDB, der 2016 veröffentlicht wurde. Diesmal verfolgte man eine neue Herangehensweise, indem man die Berechnungen nicht auf eigenschaftserhaltenden Verschlüsselungsverfahren, sondern auf speziellen Datenstrukturen aufsetzt. Dies erlaubt es, komplett unabhängig von einer Art der Verschlüsselung zu sein und ausschließlich starke bzw. anerkannte Standardverfahren, wie den \ac{AES}, zu nutzen. Arx nutzt spezielle Datenbankindizes, welche speziell zum Traversieren verschlüsselter Daten entworfen wurden und die Sicherheit dahingehend gewähren, dass die Knoten in den Indexstrukturen nach jeder Benutzung zerstört und entsprechend erneuert werden müssen.

In diesem Abschnitt wurden die zwei generellen Ansätze zur Umsetzung eines sicheren Datenbanksystems und die wichtigsten Vertreter auf beiden Seiten betrachtet. Der Einsatz von dedizierter Hardware als sicherer Vertrauensbereich zur Datenverarbeitung erlaubt eine große Flexibilität in Bezug auf Konfigurierbarkeit und verfügbare Operationen. Demgegenüber stehen die mit oftmals hohen Kosten verbundene Integration jener Komponenten in das Gesamtsystem sowie die begrenzten sicheren Speicher- und Rechenkapazitäten als Flaschenhals. Die Nutzung von eigenschaftserhaltenden Verschlüsselungsschemata und speziellen Datenstrukturen hingegen erlaubt eine schnellere Lösung in Software, allerdings auf Kosten des Sicherheitsgrades und der Funktionalität. Die Kombination beider Ansätze, etwa in Cipherbase, sorgt dafür, dass nur ein möglichst kleiner Teil an Berechnungen in der sicheren Hardware ausgeführt wird und bietet somit einen Kompromiss der jeweiligen Vor- und Nachteile.

\section{Intel SGX im Kontext sicherer Datenbanken}

Nach einem Überblick über bestehende Ansätze zur Lösung des Problems eines sicheren Umgangs mit Daten in einer Datenbank, wird nun die in dieser Arbeit zentrale Technologie vorgestellt. Wie auch schon bei den Vertretern TrustedDB und Cipherbase steht der Einsatz einer vertrauenswürdigen Hardwarekomponente im Mittelpunkt, mit der Absicht, eine sichere Datenspeicherung und -verarbeitung in einem potenziell böswilligen Umfeld durchführen zu können. Während die zuvor genannten Ansätze jedoch die Integration zusätzlicher Hardware voraussetzten, geschieht dies in diesem Fall komplett auf einem gebräuchlichen Intel Prozessor. Erstmals 2013 von Intel beschrieben \cite{McKeen2013}, sind die als \ac{SGX} veröffentlichten x86 Architekturerweiterungen im Herbst 2015 als Teil der 6. Generation Intel Core Prozessoren (Codename Skylake) erschienen \cite{Aumasson2016}. In diesem Abschnitt soll ein Überblick über das Konzept und die allgemeine Funktionsweise von Intel \ac{SGX} gegeben werden. Zunächst wird dabei auf die technischen Grundbestandteile eingegangen. Ein besonderes Merkmal von \ac{SGX} ist die Möglichkeit der \textit{remote attestation}, welche es einem Client ermöglicht, sicherzustellen, dass er auch wirklich mit dem beabsichtigtem Teil der Software in einem sicheren Bereich auf dem entfernten Server interagiert.

Die Grundlage der Funktionalität von \ac{SGX} ist die Definition einer eigenen, kontinuierlichen Region im physischen Hauptspeicher des entsprechenden Systems. Diese wird als \ac{PRM} bezeichnet und beherbergt die sogenannten Enclaves, sichere Einheiten zur Verwahrung von sensiblem Code und Daten. Jede Enclave kann dabei als ein abgeschlossener, verschlüsselter Container betrachtet werden. Die \ac{CPU} schützt den \ac{PRM} vor sämtlichen Zugriffen, welche nicht von einer Enclave ausgehen, sei es von Seiten des Kernels, Hypervisors, durch Peripheriegeräte oder sonstiger (privilegierter) Systemkomponenten. Die Speicherseiten der einzelnen Enclaves sind im \ac{EPC} verwahrt, welcher im \ac{PRM} beinhaltet ist. Metadaten zu jeder \ac{EPC} Seite werden in einem \ac{EPCM}, einer Datenstruktur im Prozessor selbst, mitgeführt. Ein Schema, welches die eben eingeführten Komponenten zusammenhängend darstellt, ist in Abbildung \ref{fig:intelsgxmemory1} zu sehen.

\begin{figure}
	\includegraphics[width=0.9\linewidth]{img/IntelSGXMemory1.pdf}
	\centering
	\caption{Zusammenhang von PRM, EPC und EPCM}
	\label{fig:intelsgxmemory1}
\end{figure}

Die Anwendungssoftware kann eine oder mehrere Enclaves einbinden, um sie zur sicheren Datenspeicherung und Berechnung zu nutzen. Dieser Vorgang beginnt zunächst mit einer Einrichtung der jeweiligen Enclave. Deren initialer Inhalt wird zunächst von der \ac{CPU} in den \ac{PRM} transferiert, woraufhin die nicht vertrauenswürdige Systemsoftware jene \ac{EPC} Seiten alloziert und der zu ladenden Enclave zuordnet. Um zu verhindern, dass die Sicherheitsgarantien trotz Einbeziehung des Betriebssystems eingehalten werden können, wird im \ac{EPCM} vermerkt, welcher Enclave jede EPC Seite angehört. Der Prozessor kann somit prüfen, ob die Allokation korrekt durchgeführt wurde, oder etwa eine Seite mehrfach zugeteilt wurde. Die jeweils erste allozierte Seite dient einer Unterbringung der \ac{SECS}, welche Metadaten der entsprechenden Enclave beinhaltet und ab diesem Zeitpunkt zu deren Referenzierung dient. Nach Vollendung der Prozedur wird die Enclave als initialisiert gekennzeichnet und der sogenannte \textit{Measurement Hash} vollendet, ein kryptographischer Hash des Enclaveinhalts, welcher zu Zwecken der Attestierung benötigt wird. Außerdem am Startprozess beteiligt ist stets die sogenannte \textit{Launch Enclave}, welche durch Intel für das Laden anderer Enclaves bereitgestellt wird und als Teil eines Systemdienstes im Hintergrund läuft. Ihr zusätzlicher Zweck ist es, den Start einer Enclave zu autorisieren. Im produktiven Umfeld eingesetzte Enclaves müssen sich nämlich einer offiziellen Lizenzierung durch Intel unterziehen \cite{Costan2016}.

Nach dem Laden der Enclave befindet sie sich im \ac{PRM} und Adressraum des jeweiligen Nutzerprozesses. Innerhalb der einsatzbereiten Enclave befinden sich nun ihr eigener privater Code und private Daten. Das vorliegende Adressraumschema ist vereinfacht in Abbildung \ref{fig:intelsgxmemory2} gezeigt. Jegliche Berechnungen sowie gespeicherte Daten innerhalb der Grenzen einer einzelnen Enclave sind durch die \ac{CPU} geschützt. Sämtlicher Inhalt ist zudem durch eine oder mehrere dedizierte Hardwarekomponenten, sogenannten \textit{Memory Encryption Engines}, verschlüsselt und kann nur innerhalb der \ac{CPU} entschlüsselt werden, was es unmöglich macht, Informationen durch das Auslesen des Speichers zu gewinnen \cite{McKeen2013}. Sobald ihre Funktionalität beansprucht wird, sprich der Ausführungsfluss die Enclave passieren will, sind spezielle \ac{CPU} Instruktionen notwendig. Der hierfür eingesetzte Mechanismus ist ähnlich dem Sprung vom User- in den Kernel Mode. \ac{SGX} unterstützt zudem Multithreading, wodurch sich mehrere Threads parallel in einer Enclave befinden können. Je zu unterstützendem logischen Prozessor erfordert dies das Anlegen von Metadaten in einer \ac{TCS}, welche jeweils eine \ac{EPC} Seite füllt. Der Zweck liegt darin, dass der Ausführungspfad eines Threads innerhalb der Enclave genauestens bekannt und somit dokumentiert sein muss, um ihn in den Measurement Hash eingehen zu lassen \cite{McKeen2013}.

\begin{figure}
	\includegraphics[width=0.6\linewidth]{img/IntelSGXMemory2.pdf}
	\centering
	\caption{Adressraumschema nach Einbindung sicherer Enclaves}
	\label{fig:intelsgxmemory2}
\end{figure}

Da die Enclave selbst aus dem unsicheren Speicher geladen wird, ist ihr initialer Zustand nach außen hin bekannt. Die sichere Verwendung findet folglich erst nach der Initialisierung statt. Es ist jedoch im Interesse eines Nutzers zu wissen, ob das Stück Software auf dem entfernten System, welches seine Daten bearbeiten soll, auch seinen Erwartungen entspricht. \ac{SGX} erlaubt es, mithilfe der remote attestation eine Integritätsprüfung auf dem Inhalt der Enclave durchführen zu lassen. Hierbei kommt ein asymmetrisches Verschlüsselungsverfahren zum Signieren ihres Measurements zum Einsatz \cite{Costan2016}. Der Nutzer kann diese Signatur prüfen und somit die Integrität der Gegenseite bewerten. Die für den ganzen Vorgang benötigten \textit{Attestation Keys} werden von Intel selbst ausgestellt \cite{Johnson2016}.

Der sicherlich größte Unterschied zu allen zuvor betrachteten Entwicklungen, welche auf Vertrauensbereiche in Hardware setzen, ist die Einbeziehung bereits in jedem System vorhandener Hardwarekomponenten, genauer gesagt des Hauptspeichers und vor allem des Prozessors. Letzterer bildet somit, zusätzlich zu sämtlichen zur Attestierung notwendigen Softwarebestandteilen, die allgemeine \ac{TCB}. Es muss somit auf die korrekte Funktionsweise des Intel Prozessors vertraut werden, was allerdings zwangsläufig der Fall ist, da er für die Ausführung von Programmen zuständig ist und seine interne Logik ohnehin schwer überprüft werden kann \cite{Aumasson2016}. Unter Nutzung von \ac{SGX} können bereits vorhandene Systeme ohne eine große Aufrüstung durch zusätzliche Hardware um eine Komponente zur sicheren Datenverarbeitung erweitert werden. Interessant ist hierbei natürlich, welche Eigenschaften die Enclaves in Bezug auf zur Verfügung stehendem Speicher, Leistungsaufwand bei Berechnungen oder Möglichkeiten bei der Programmierung aufzeigen. Entsprechende Untersuchungen werden als Hauptaugenmerk dieser Arbeit in späteren Kapiteln betrachtet.

\section{Fazit}

In diesem Kapitel wurde die Datensicherheit aus Sicht eines Datenbanksystems behandelt. Um die drei Schutzziele Vertraulichkeit, Integrität und Verfügbarkeit zu gewährleisten, wurde klassischerweise auf ein hierarchisch organisiertes Prinzip gesetzt. Der physische Schutz des Systems bildet die Grundlage der Sicherheit des Betriebssystems, welche wiederum erforderlich ist, um das Datenbanksystem zu schützen. Jede Schicht verfügt über eigene Maßnahmen, um dieses Ziel zu erreichen. Im Kontext von cloudbasierten Datenbanken ist jenes konventionelle Prinzip jedoch nicht mehr ausreichend, da sowohl die physische Hardware, als auch das Betriebssystem unter der Kontrolle eines Dritten stehen. Um dennoch die Vertraulichkeit und Integrität von Data-at-rest und Data-in-use garantieren zu können, ergeben sich gewisse Anforderungen an das Datenbanksystem. Ruhende Daten müssen in verschlüsselter Form vorliegen und die Datenverarbeitung muss auf eine Weise stattfinden, dass keine Klartextdaten von einem Angreifer ausgelesen werden können. In der Vergangenheit haben sich zwei allgemeine Ansätze gebildet, um ein derartiges System zu entwerfen. In Vertretern wie TrustedDB und Cipherbase setzt man vorrangig auf die Einbeziehung von zusätzlicher Hardware, um Bereiche zu schaffen, in denen gesicherte Berechnungen auf Klartexten stattfinden können. Publikationen zu CryptDB oder Arx hingegen nutzen lediglich spezielle Verschlüsselungsschemata und Datenstrukturen, um verarbeitende Operationen direkt auf Schlüsseltexten durchzuführen. Intel \ac{SGX} ist eine aktuelle Lösung des Prozessorherstellers, welche den Ansatz der Vertrauensbereiche in einen handelsüblichen Prozessor integriert. Dieser nutzt einen gesicherten Bereich im Hauptspeicher und schützt ihn vor sämtlichen Zugriffen von außen, inklusive jener von privilegierten Komponenten wie dem Kernel oder Hypervisor.